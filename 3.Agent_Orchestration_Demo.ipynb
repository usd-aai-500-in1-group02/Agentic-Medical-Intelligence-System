{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Medical System Orchestration Demonstration\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook demonstrates the **Agent Orchestration System** of our Agentic Medical Intelligence System, replicating the behavior of `agent_decision.py` using **LangGraph** for intelligent agent routing and workflow management. This system represents the core decision-making engine that coordinates between specialized medical AI agents to provide comprehensive healthcare assistance.\n",
        "\n",
        "### LangGraph Agent Orchestration Architecture:\n",
        "\n",
        "Our system employs a sophisticated multi-agent architecture powered by LangGraph, featuring:\n",
        "\n",
        "1. **CONVERSATION_AGENT**: General medical consultation and patient interaction\n",
        "2. **RAG_AGENT**: Medical literature retrieval from pre-ingested knowledge base\n",
        "3. **WEB_SEARCH_PROCESSOR_AGENT**: Current medical information and recent developments\n",
        "4. **BRAIN_TUMOR_AGENT**: YOLO-based brain MRI tumor detection and visualization\n",
        "5. **CHEST_XRAY_AGENT**: DenseNet COVID-19 detection in chest X-rays\n",
        "6. **SKIN_LESION_AGENT**: U-Net skin lesion segmentation and analysis\n",
        "\n",
        "### Key Orchestration Features:\n",
        "\n",
        "- **Intelligent Routing**: Automatic agent selection based on query content and context\n",
        "- **Direct Image Routing**: Bypass expensive LLM calls for classified medical images\n",
        "- **Confidence-Based Fallbacks**: RAG ‚Üí Web Search routing when local knowledge insufficient\n",
        "- **Human Validation Workflows**: Medical diagnosis validation and oversight\n",
        "- **Guardrails Integration**: Input/output safety and ethical content filtering\n",
        "- **Conversation Memory**: Persistent state management across interactions\n",
        "\n",
        "### Academic Significance:\n",
        "\n",
        "This demonstration showcases advanced concepts in:\n",
        "- **Multi-Agent Systems**: Coordination and communication between specialized AI agents\n",
        "- **Workflow Orchestration**: LangGraph state management and conditional routing\n",
        "- **Medical AI Integration**: Combining local models with cloud-based reasoning\n",
        "- **Decision Tree Logic**: Transparent, explainable agent selection algorithms\n",
        "- **Human-AI Collaboration**: Validation workflows for critical medical decisions\n",
        "\n",
        "---\n",
        "\n",
        "**Ethics Statement**: This system is designed for educational and research purposes. All medical decisions require validation by qualified healthcare professionals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies\n",
        "\n",
        "This section initializes the comprehensive environment required for multi-agent orchestration. Our technical stack includes:\n",
        "\n",
        "### Core Orchestration Frameworks:\n",
        "- **LangGraph**: Advanced workflow orchestration and state management\n",
        "- **LangChain**: LLM integration and prompt engineering\n",
        "- **Qdrant**: Vector database for medical literature retrieval\n",
        "- **OpenAI GPT**: Decision-making and natural language processing\n",
        "\n",
        "### Medical AI Integration:\n",
        "- **Custom Agent Classes**: Specialized medical AI model wrappers\n",
        "- **Guardrails System**: Safety and ethical content validation\n",
        "- **Configuration Management**: Centralized system settings and parameters\n",
        "\n",
        "### Visualization and Monitoring:\n",
        "- **Matplotlib**: Agent decision flow visualization\n",
        "- **NetworkX**: LangGraph structure representation\n",
        "- **Logging**: Comprehensive system monitoring and debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-agent orchestration environment initialized successfully\n",
            "üìç Working directory: /Users/sourangshupal/Downloads/Agentic-Medical-Intelligence-System\n",
            "üêç Python version: 3.11.6\n",
            "‚è∞ Session started: 2025-08-11 14:57:20\n",
            "üìù Logging enabled: agent_orchestration.log\n"
          ]
        }
      ],
      "source": [
        "# Import essential libraries for multi-agent orchestration\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "from typing import Dict, List, Optional, Any, Literal, TypedDict, Union, Annotated\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress warnings for cleaner academic presentation\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LangGraph and LangChain components for agent orchestration\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langgraph.graph import MessagesState, StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Environment and configuration management\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from datetime import datetime\n",
        "\n",
        "# Add project root to Python path for agent imports\n",
        "sys.path.append('/Users/sourangshupal/Downloads/Agentic-Medical-Intelligence-System')\n",
        "\n",
        "# Load environment variables containing API keys\n",
        "load_dotenv()\n",
        "\n",
        "# Configure logging for agent orchestration monitoring\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),  # Console output\n",
        "        logging.FileHandler('agent_orchestration.log', mode='a')  # File logging\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger('AgentOrchestration')\n",
        "\n",
        "# Configure matplotlib for high-quality academic visualizations\n",
        "plt.rcParams['figure.figsize'] = (14, 10)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "\n",
        "print(\"‚úÖ Multi-agent orchestration environment initialized successfully\")\n",
        "print(f\"üìç Working directory: {os.getcwd()}\")\n",
        "print(f\"üêç Python version: {sys.version.split()[0]}\")\n",
        "print(f\"‚è∞ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üìù Logging enabled: agent_orchestration.log\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,280 - agents.image_analysis_agent.brain_tumor_agent.brain_tumor_inference - INFO - Using device: cpu\n",
            "2025-08-11 14:57:25,282 - agents.image_analysis_agent.skin_lesion_agent.skin_lesion_inference - INFO - Using device: cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Medical agent orchestration components imported successfully\n",
            "\n",
            "üìã System Configuration Loaded:\n",
            "   üß† Decision Model: client=<openai.resources.chat.completions.completions.Completions object at 0x334b63990> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x33555a050> root_client=<openai.OpenAI object at 0x334b6f910> root_async_client=<openai.AsyncOpenAI object at 0x333ff9050> model_name='gpt-4' temperature=0.1 model_kwargs={} openai_api_key=SecretStr('**********')\n",
            "   üëÅÔ∏è Vision Model: GPT-4o Vision for image classification\n",
            "   üìö RAG Collection: medirag\n",
            "   üîç Web Search: Tavily API integration\n",
            "   üõ°Ô∏è Guardrails: Local safety validation enabled\n",
            "   üíæ Memory: LangGraph conversation persistence\n"
          ]
        }
      ],
      "source": [
        "# Import specialized medical agent components\n",
        "# These represent the core medical AI capabilities of our system\n",
        "\n",
        "try:\n",
        "    from config import Config\n",
        "    from agents.rag_agent import MedicalRAG\n",
        "    from agents.web_search_processor_agent import WebSearchProcessorAgent\n",
        "    from agents.image_analysis_agent import ImageAnalysisAgent\n",
        "    from agents.guardrails.local_guardrails import LocalGuardrails\n",
        "    \n",
        "    # Initialize system configuration with all agent settings\n",
        "    config = Config()\n",
        "    \n",
        "    print(\"üî¨ Medical agent orchestration components imported successfully\")\n",
        "    print(\"\\nüìã System Configuration Loaded:\")\n",
        "    print(f\"   üß† Decision Model: {config.agent_decision.llm}\")\n",
        "    print(f\"   üëÅÔ∏è Vision Model: GPT-4o Vision for image classification\")\n",
        "    print(f\"   üìö RAG Collection: {config.rag.collection_name}\")\n",
        "    print(f\"   üîç Web Search: Tavily API integration\")\n",
        "    print(f\"   üõ°Ô∏è Guardrails: Local safety validation enabled\")\n",
        "    print(f\"   üíæ Memory: LangGraph conversation persistence\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing agent components: {e}\")\n",
        "    print(\"Please ensure all medical agent modules are available\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Configuration error: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Agent State and Configuration Classes\n",
        "\n",
        "This section defines the foundational data structures for our multi-agent system. The design follows enterprise-grade software architecture principles:\n",
        "\n",
        "### AgentState Class:\n",
        "Extends LangGraph's `MessagesState` to track:\n",
        "- **Conversation History**: Complete message chain with context preservation\n",
        "- **Agent Metadata**: Current active agent and routing decisions\n",
        "- **Input Processing**: Text, image, and multi-modal input handling\n",
        "- **Validation Flags**: Human oversight requirements for medical decisions\n",
        "- **Confidence Metrics**: Quantitative assessment of response reliability\n",
        "\n",
        "### AgentConfig Class:\n",
        "Centralizes system parameters:\n",
        "- **Model Selection**: GPT-4o for decision-making and vision analysis\n",
        "- **Confidence Thresholds**: Evidence-based routing criteria\n",
        "- **System Prompts**: Carefully engineered instructions for medical triage\n",
        "- **Agent Routing Logic**: Transparent decision-making algorithms\n",
        "\n",
        "### Clinical Decision-Making Algorithms:\n",
        "Our routing system implements clinical triage principles:\n",
        "1. **Image Priority**: Medical images receive immediate specialized routing\n",
        "2. **Knowledge Hierarchy**: Local literature ‚Üí Web search ‚Üí Human validation\n",
        "3. **Safety First**: All medical diagnoses require human oversight\n",
        "4. **Confidence Thresholds**: Statistical validation of response reliability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,658 - AgentOrchestration - INFO - Agent state and configuration classes successfully defined\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Agent State and Configuration Classes Defined\n",
            "   üéØ Decision Model: gpt-4o\n",
            "   üìä Confidence Threshold: 0.85\n",
            "   üî¨ Available Agents: 6 specialized medical agents\n",
            "   üõ°Ô∏è Safety Features: Guardrails and human validation integrated\n"
          ]
        }
      ],
      "source": [
        "# Define the comprehensive agent state structure for multi-agent coordination\n",
        "# This extends LangGraph's MessagesState with specialized medical workflow requirements\n",
        "\n",
        "class AgentState(MessagesState):\n",
        "    \"\"\"\n",
        "    Extended state management for medical multi-agent system.\n",
        "    \n",
        "    This class maintains comprehensive state information across the agent workflow,\n",
        "    including conversation history, agent metadata, validation requirements,\n",
        "    and confidence metrics essential for medical decision-making.\n",
        "    \n",
        "    Attributes:\n",
        "        messages: List[BaseMessage] - Complete conversation history (inherited)\n",
        "        agent_name: Current active agent identifier\n",
        "        current_input: Raw user input (text, dict with image, or multi-modal)\n",
        "        has_image: Boolean flag for image presence detection\n",
        "        image_type: Medical image classification (MRI, X-ray, dermoscopy)\n",
        "        output: Final response content for user delivery\n",
        "        needs_human_validation: Medical oversight requirement flag\n",
        "        retrieval_confidence: RAG system confidence score (0.0-1.0)\n",
        "        bypass_routing: Guardrails bypass flag for safety violations\n",
        "        insufficient_info: RAG knowledge gap indicator for web search routing\n",
        "    \"\"\"\n",
        "    # Core agent workflow state\n",
        "    agent_name: Optional[str]  # Current processing agent identifier\n",
        "    current_input: Optional[Union[str, Dict]]  # User input to be processed\n",
        "    \n",
        "    # Image processing state\n",
        "    has_image: bool  # Flag indicating image upload presence\n",
        "    image_type: Optional[str]  # Classified medical image type for routing\n",
        "    \n",
        "    # Response and validation state\n",
        "    output: Optional[str]  # Generated response for user delivery\n",
        "    needs_human_validation: bool  # Medical diagnosis validation requirement\n",
        "    \n",
        "    # Quality and routing metrics\n",
        "    retrieval_confidence: float  # RAG retrieval confidence score\n",
        "    bypass_routing: bool  # Safety guardrails bypass flag\n",
        "    insufficient_info: bool  # Knowledge gap indicator for routing decisions\n",
        "\n",
        "\n",
        "class AgentDecision(TypedDict):\n",
        "    \"\"\"\n",
        "    Structured output format for agent routing decisions.\n",
        "    \n",
        "    This standardized format ensures consistent decision-making across\n",
        "    all routing scenarios, providing transparency and auditability\n",
        "    for clinical decision support systems.\n",
        "    \n",
        "    Attributes:\n",
        "        agent: Target agent identifier for query routing\n",
        "        reasoning: Step-by-step explanation of routing decision\n",
        "        confidence: Quantitative confidence score (0.0-1.0)\n",
        "    \"\"\"\n",
        "    agent: str  # Target agent for query processing\n",
        "    reasoning: str  # Transparent decision-making rationale\n",
        "    confidence: float  # Statistical confidence in routing decision\n",
        "\n",
        "\n",
        "class AgentConfig:\n",
        "    \"\"\"\n",
        "    Comprehensive configuration settings for medical agent orchestration system.\n",
        "    \n",
        "    This class centralizes all system parameters, model selections, and routing logic\n",
        "    to ensure consistent behavior across the multi-agent medical workflow.\n",
        "    Clinical decision-making thresholds are based on evidence-based medicine principles.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Model configuration for decision-making and analysis\n",
        "    DECISION_MODEL = \"gpt-4o\"  # Primary LLM for agent routing decisions\n",
        "    VISION_MODEL = \"gpt-4o\"   # Vision model for medical image classification\n",
        "    \n",
        "    # Clinical decision-making thresholds\n",
        "    CONFIDENCE_THRESHOLD = 0.85  # Minimum confidence for automated routing\n",
        "    \n",
        "    # Comprehensive system prompt for intelligent medical triage\n",
        "    DECISION_SYSTEM_PROMPT = \"\"\"You are an intelligent medical triage system that routes user queries to \n",
        "    the appropriate specialized agent. Your job is to analyze the user's request and determine which agent \n",
        "    is best suited to handle it based on the query content, presence of images, and conversation context.\n",
        "\n",
        "    Available agents:\n",
        "    1. CONVERSATION_AGENT - For general chat, greetings, and non-medical questions.\n",
        "    2. RAG_AGENT - For specific medical knowledge questions that can be answered from established medical literature. \n",
        "       Currently ingested medical knowledge involves 'introduction to brain tumor', 'deep learning techniques to \n",
        "       diagnose and detect brain tumors', 'deep learning techniques to diagnose and detect covid / covid-19 from chest x-ray'.\n",
        "    3. WEB_SEARCH_PROCESSOR_AGENT - For questions about recent medical developments, current outbreaks, or time-sensitive medical information.\n",
        "    4. BRAIN_TUMOR_AGENT - For analysis of brain MRI images to detect and segment tumors.\n",
        "    5. CHEST_XRAY_AGENT - For analysis of chest X-ray images to detect abnormalities.\n",
        "    6. SKIN_LESION_AGENT - For analysis of skin lesion images to classify them as benign or malignant.\n",
        "\n",
        "    Make your decision based on these guidelines:\n",
        "    - If the user has not uploaded any image, always route to the conversation agent.\n",
        "    - If the user uploads a medical image, decide which medical vision agent is appropriate based on the image type and the user's query. \n",
        "      If the image is uploaded without a query, always route to the correct medical vision agent based on the image type.\n",
        "    - If the user asks about recent medical developments or current health situations, use the web search processor agent.\n",
        "    - If the user asks specific medical knowledge questions, use the RAG agent.\n",
        "    - For general conversation, greetings, or non-medical questions, use the conversation agent. \n",
        "      But if image is uploaded, always go to the medical vision agents first.\n",
        "\n",
        "    You must provide your answer in JSON format with the following structure:\n",
        "    {{\n",
        "    \"agent\": \"AGENT_NAME\",\n",
        "    \"reasoning\": \"Your step-by-step reasoning for selecting this agent\",\n",
        "    \"confidence\": 0.95  // Value between 0.0 and 1.0 indicating your confidence in this decision\n",
        "    }}\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize image analysis capability for medical image routing\n",
        "    @classmethod\n",
        "    def get_image_analyzer(cls):\n",
        "        \"\"\"Get initialized image analysis agent for medical image classification.\"\"\"\n",
        "        if not hasattr(cls, '_image_analyzer'):\n",
        "            cls._image_analyzer = ImageAnalysisAgent(config=config)\n",
        "        return cls._image_analyzer\n",
        "\n",
        "print(\"üìã Agent State and Configuration Classes Defined\")\n",
        "print(f\"   üéØ Decision Model: {AgentConfig.DECISION_MODEL}\")\n",
        "print(f\"   üìä Confidence Threshold: {AgentConfig.CONFIDENCE_THRESHOLD}\")\n",
        "print(f\"   üî¨ Available Agents: 6 specialized medical agents\")\n",
        "print(f\"   üõ°Ô∏è Safety Features: Guardrails and human validation integrated\")\n",
        "logger.info(\"Agent state and configuration classes successfully defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LangGraph Workflow Construction\n",
        "\n",
        "This section implements the core LangGraph workflow that orchestrates our multi-agent medical system. The architecture follows enterprise-grade workflow patterns:\n",
        "\n",
        "### Workflow Architecture Components:\n",
        "\n",
        "#### **3.1 Input Analysis Node**\n",
        "- **Image Detection**: Automatic medical image type classification\n",
        "- **Guardrails Check**: Input safety validation and content filtering\n",
        "- **Context Preparation**: Conversation history and state initialization\n",
        "\n",
        "#### **3.2 Intelligent Routing System**\n",
        "- **Direct Medical Image Routing**: Bypass expensive LLM calls for classified images\n",
        "- **OpenAI Decision Chain**: Advanced reasoning for complex query types\n",
        "- **Confidence-Based Decisions**: Statistical validation of routing choices\n",
        "\n",
        "#### **3.3 Agent Execution Nodes**\n",
        "- **Parallel Processing**: Independent agent execution with state management\n",
        "- **Error Handling**: Graceful failure recovery and fallback mechanisms\n",
        "- **Performance Monitoring**: Execution time tracking and optimization\n",
        "\n",
        "#### **3.4 Validation and Safety Pipeline**\n",
        "- **Human Validation**: Medical diagnosis oversight requirements\n",
        "- **Output Guardrails**: Response safety and appropriateness checking\n",
        "- **Quality Assurance**: Confidence scoring and accuracy validation\n",
        "\n",
        "### Clinical Decision-Making Flow:\n",
        "The workflow implements evidence-based medical triage principles:\n",
        "1. **Immediate Safety Check**: Input validation and content filtering\n",
        "2. **Image Priority Routing**: Direct routing for classified medical images\n",
        "3. **Knowledge Hierarchy**: Local ‚Üí Web ‚Üí Human validation escalation\n",
        "4. **Quality Control**: Confidence thresholds and validation requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,668 - AgentOrchestration - INFO - Initializing medical agent orchestration graph\n",
            "2025-08-11 14:57:25,669 - AgentOrchestration - INFO - Medical agent graph components successfully initialized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è Building LangGraph Workflow Architecture...\n",
            "   üìä Components: Decision chain, guardrails, memory persistence\n",
            "   üîÑ Workflow: Input ‚Üí Analysis ‚Üí Routing ‚Üí Execution ‚Üí Validation\n",
            "   ‚úÖ Input Analysis Node: Image detection and guardrails validation\n"
          ]
        }
      ],
      "source": [
        "def create_medical_agent_graph():\n",
        "    \"\"\"\n",
        "    Create and configure the comprehensive LangGraph workflow for medical agent orchestration.\n",
        "    \n",
        "    This function constructs a sophisticated state graph that manages the complete\n",
        "    medical consultation workflow, from initial query analysis through specialized\n",
        "    agent execution to final response validation and delivery.\n",
        "    \n",
        "    The workflow implements clinical triage principles with automated routing,\n",
        "    safety guardrails, and human validation requirements for medical decisions.\n",
        "    \n",
        "    Returns:\n",
        "        Compiled LangGraph workflow with memory persistence\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(\"Initializing medical agent orchestration graph\")\n",
        "    \n",
        "    # Initialize core system components\n",
        "    memory = MemorySaver()  # Conversation persistence across sessions\n",
        "    guardrails = LocalGuardrails(config.rag.llm)  # Safety and content validation\n",
        "    decision_model = config.agent_decision.llm  # Primary decision-making LLM\n",
        "    json_parser = JsonOutputParser(pydantic_object=AgentDecision)  # Structured output parsing\n",
        "    \n",
        "    # Create the agent decision prompt template\n",
        "    decision_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", AgentConfig.DECISION_SYSTEM_PROMPT),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "    \n",
        "    # Construct the decision-making chain\n",
        "    decision_chain = decision_prompt | decision_model | json_parser\n",
        "    \n",
        "    print(\"üèóÔ∏è Building LangGraph Workflow Architecture...\")\n",
        "    print(\"   üìä Components: Decision chain, guardrails, memory persistence\")\n",
        "    print(\"   üîÑ Workflow: Input ‚Üí Analysis ‚Üí Routing ‚Üí Execution ‚Üí Validation\")\n",
        "    \n",
        "    # === WORKFLOW NODE DEFINITIONS ===\n",
        "    \n",
        "    def analyze_input(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Comprehensive input analysis including image detection and guardrails validation.\n",
        "        \n",
        "        This function serves as the entry point for all user interactions,\n",
        "        performing essential preprocessing steps including medical image\n",
        "        classification and content safety validation.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with user input\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with analysis results and routing flags\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting input analysis phase\")\n",
        "        \n",
        "        current_input = state[\"current_input\"]\n",
        "        has_image = False\n",
        "        image_type = None\n",
        "        \n",
        "        # Extract text content for processing\n",
        "        input_text = \"\"\n",
        "        if isinstance(current_input, str):\n",
        "            input_text = current_input\n",
        "        elif isinstance(current_input, dict):\n",
        "            input_text = current_input.get(\"text\", \"\")\n",
        "        \n",
        "        # Apply input safety guardrails validation\n",
        "        if input_text:\n",
        "            is_allowed, guardrail_message = guardrails.check_input(input_text)\n",
        "            if not is_allowed:\n",
        "                logger.warning(f\"Input blocked by guardrails: {input_text[:100]}...\")\n",
        "                print(f\"üõ°Ô∏è Input Safety Check: Content blocked by guardrails\")\n",
        "                return {\n",
        "                    **state,\n",
        "                    \"messages\": AIMessage(content=guardrail_message),\n",
        "                    \"agent_name\": \"INPUT_GUARDRAILS\",\n",
        "                    \"has_image\": False,\n",
        "                    \"image_type\": None,\n",
        "                    \"bypass_routing\": True  # Skip normal workflow\n",
        "                }\n",
        "        \n",
        "        # Process medical image if present\n",
        "        if isinstance(current_input, dict) and \"image\" in current_input:\n",
        "            has_image = True\n",
        "            image_path = current_input.get(\"image\")\n",
        "            \n",
        "            logger.info(f\"Processing medical image: {image_path}\")\n",
        "            print(f\"üì∏ Medical Image Detected: Analyzing image type...\")\n",
        "            \n",
        "            try:\n",
        "                # Classify medical image type using GPT-4o Vision\n",
        "                image_analyzer = AgentConfig.get_image_analyzer()\n",
        "                image_analysis_result = image_analyzer.analyze_image(image_path)\n",
        "                \n",
        "                # Extract image type from analysis result\n",
        "                if isinstance(image_analysis_result, dict):\n",
        "                    image_type = image_analysis_result.get('image_type', 'UNKNOWN')\n",
        "                elif isinstance(image_analysis_result, str):\n",
        "                    # Parse string response for image type\n",
        "                    image_type = \"UNKNOWN\"\n",
        "                    if \"chest x-ray\" in image_analysis_result.lower():\n",
        "                        image_type = \"CHEST X-RAY\"\n",
        "                    elif \"brain\" in image_analysis_result.lower() and \"mri\" in image_analysis_result.lower():\n",
        "                        image_type = \"BRAIN MRI\"\n",
        "                    elif \"skin\" in image_analysis_result.lower() or \"lesion\" in image_analysis_result.lower():\n",
        "                        image_type = \"SKIN LESION\"\n",
        "                \n",
        "                print(f\"üîç Image Classification Result: {image_type}\")\n",
        "                logger.info(f\"Image classified as: {image_type}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"Image analysis failed: {str(e)}\")\n",
        "                print(f\"‚ö†Ô∏è Image Analysis Warning: {str(e)}\")\n",
        "                image_type = \"UNKNOWN\"\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"has_image\": has_image,\n",
        "            \"image_type\": image_type,\n",
        "            \"bypass_routing\": False  # Continue normal workflow\n",
        "        }\n",
        "    \n",
        "    def check_routing_bypass(state: AgentState) -> str:\n",
        "        \"\"\"\n",
        "        Determine if normal agent routing should be bypassed due to guardrails.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state\n",
        "            \n",
        "        Returns:\n",
        "            Routing decision: 'apply_guardrails' or 'route_to_agent'\n",
        "        \"\"\"\n",
        "        if state.get(\"bypass_routing\", False):\n",
        "            return \"apply_guardrails\"\n",
        "        return \"route_to_agent\"\n",
        "    \n",
        "    print(\"   ‚úÖ Input Analysis Node: Image detection and guardrails validation\")\n",
        "    return decision_chain, analyze_input, check_routing_bypass\n",
        "\n",
        "# Initialize the workflow construction\n",
        "decision_chain, analyze_input, check_routing_bypass = create_medical_agent_graph()\n",
        "logger.info(\"Medical agent graph components successfully initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,678 - AgentOrchestration - INFO - Intelligent routing system successfully initialized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Intelligent Routing System: Direct image routing + OpenAI decision chain\n"
          ]
        }
      ],
      "source": [
        "# Continue building the LangGraph workflow with intelligent routing system\n",
        "\n",
        "def create_routing_system():\n",
        "    \"\"\"\n",
        "    Create the sophisticated routing system that determines optimal agent selection.\n",
        "    \n",
        "    This system implements a two-tier routing approach:\n",
        "    1. Direct routing for classified medical images (efficiency optimization)\n",
        "    2. OpenAI decision chain for complex queries (reasoning optimization)\n",
        "    \n",
        "    Returns:\n",
        "        Routing function for agent selection\n",
        "    \"\"\"\n",
        "    \n",
        "    def route_to_agent(state: AgentState) -> Dict:\n",
        "        \"\"\"\n",
        "        Intelligent agent routing based on input analysis and clinical triage principles.\n",
        "        \n",
        "        This function implements a sophisticated decision tree that routes queries\n",
        "        to the most appropriate agent based on content analysis, image presence,\n",
        "        and conversation context. The routing logic prioritizes efficiency and\n",
        "        clinical accuracy while maintaining cost-effective operation.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with input and analysis results\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with updated state and routing decision\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting intelligent agent routing\")\n",
        "        \n",
        "        messages = state[\"messages\"]\n",
        "        current_input = state[\"current_input\"]\n",
        "        has_image = state[\"has_image\"]\n",
        "        image_type = state[\"image_type\"]\n",
        "        \n",
        "        # === DIRECT ROUTING FOR MEDICAL IMAGES ===\n",
        "        # This optimization bypasses expensive LLM calls for classified medical images\n",
        "        if has_image and image_type:\n",
        "            image_type_upper = image_type.upper()\n",
        "            \n",
        "            # Direct routing to specialized medical vision agents\n",
        "            if \"CHEST X-RAY\" in image_type_upper or \"CHEST_XRAY\" in image_type_upper:\n",
        "                logger.info(\"Direct routing: Chest X-ray ‚Üí CHEST_XRAY_AGENT\")\n",
        "                print(f\"ü´Å Direct Routing: Chest X-ray detected ‚Üí CHEST_XRAY_AGENT\")\n",
        "                updated_state = {**state, \"agent_name\": \"CHEST_XRAY_AGENT\"}\n",
        "                return {\"agent_state\": updated_state, \"next\": \"CHEST_XRAY_AGENT\"}\n",
        "            \n",
        "            elif \"BRAIN MRI\" in image_type_upper or \"BRAIN_TUMOR\" in image_type_upper:\n",
        "                logger.info(\"Direct routing: Brain MRI ‚Üí BRAIN_TUMOR_AGENT\")\n",
        "                print(f\"üß† Direct Routing: Brain MRI detected ‚Üí BRAIN_TUMOR_AGENT\")\n",
        "                updated_state = {**state, \"agent_name\": \"BRAIN_TUMOR_AGENT\"}\n",
        "                return {\"agent_state\": updated_state, \"next\": \"BRAIN_TUMOR_AGENT\"}\n",
        "            \n",
        "            elif \"SKIN LESION\" in image_type_upper:\n",
        "                logger.info(\"Direct routing: Skin lesion ‚Üí SKIN_LESION_AGENT\")\n",
        "                print(f\"üë§ Direct Routing: Skin lesion detected ‚Üí SKIN_LESION_AGENT\")\n",
        "                updated_state = {**state, \"agent_name\": \"SKIN_LESION_AGENT\"}\n",
        "                return {\"agent_state\": updated_state, \"next\": \"SKIN_LESION_AGENT\"}\n",
        "            \n",
        "            elif \"NON-MEDICAL\" in image_type_upper or \"OTHER\" in image_type_upper:\n",
        "                logger.info(\"Direct routing: Non-medical image ‚Üí CONVERSATION_AGENT\")\n",
        "                print(f\"üí¨ Direct Routing: Non-medical image ‚Üí CONVERSATION_AGENT\")\n",
        "                updated_state = {**state, \"agent_name\": \"CONVERSATION_AGENT\"}\n",
        "                return {\"agent_state\": updated_state, \"next\": \"CONVERSATION_AGENT\"}\n",
        "        \n",
        "        # === OPENAI DECISION CHAIN FOR COMPLEX QUERIES ===\n",
        "        # For non-image queries or unclassified images, use advanced reasoning\n",
        "        logger.info(\"Using OpenAI decision chain for complex query routing\")\n",
        "        print(f\"ü§ñ Advanced Routing: Using OpenAI decision chain for query analysis\")\n",
        "        \n",
        "        # Prepare input text for decision model\n",
        "        input_text = \"\"\n",
        "        if isinstance(current_input, str):\n",
        "            input_text = current_input\n",
        "        elif isinstance(current_input, dict):\n",
        "            input_text = current_input.get(\"text\", \"\")\n",
        "        \n",
        "        # Create conversation context from recent message history\n",
        "        recent_context = \"\"\n",
        "        for msg in messages[-6:]:  # Last 3 exchanges (6 messages)\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                recent_context += f\"User: {msg.content}\\n\"\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                recent_context += f\"Assistant: {msg.content}\\n\"\n",
        "        \n",
        "        # Construct comprehensive decision input\n",
        "        decision_input = f\"\"\"\n",
        "        User query: {input_text}\n",
        "\n",
        "        Recent conversation context:\n",
        "        {recent_context}\n",
        "\n",
        "        Has image: {has_image}\n",
        "        Image type: {image_type if has_image else 'None'}\n",
        "\n",
        "        Based on this information, which agent should handle this query?\n",
        "        \"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Execute decision chain with comprehensive input\n",
        "            decision = decision_chain.invoke({\"input\": decision_input})\n",
        "            \n",
        "            agent_name = decision[\"agent\"]\n",
        "            confidence = decision.get(\"confidence\", 0.0)\n",
        "            reasoning = decision.get(\"reasoning\", \"No reasoning provided\")\n",
        "            \n",
        "            logger.info(f\"Decision made: {agent_name} (confidence: {confidence:.2f})\")\n",
        "            print(f\"üéØ Routing Decision: {agent_name}\")\n",
        "            print(f\"üìä Confidence Score: {confidence:.2f}\")\n",
        "            print(f\"üí≠ Reasoning: {reasoning[:100]}{'...' if len(reasoning) > 100 else ''}\")\n",
        "            \n",
        "            # Update state with decision results\n",
        "            updated_state = {**state, \"agent_name\": agent_name}\n",
        "            \n",
        "            # Route based on confidence threshold\n",
        "            if confidence < AgentConfig.CONFIDENCE_THRESHOLD:\n",
        "                logger.warning(f\"Low confidence routing: {confidence:.2f} < {AgentConfig.CONFIDENCE_THRESHOLD}\")\n",
        "                print(f\"‚ö†Ô∏è Low Confidence: Routing to validation workflow\")\n",
        "                return {\"agent_state\": updated_state, \"next\": \"needs_validation\"}\n",
        "            \n",
        "            return {\"agent_state\": updated_state, \"next\": agent_name}\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Decision chain failed: {str(e)}\")\n",
        "            print(f\"‚ùå Routing Error: Falling back to CONVERSATION_AGENT\")\n",
        "            # Fallback to conversation agent on decision failure\n",
        "            updated_state = {**state, \"agent_name\": \"CONVERSATION_AGENT\"}\n",
        "            return {\"agent_state\": updated_state, \"next\": \"CONVERSATION_AGENT\"}\n",
        "    \n",
        "    return route_to_agent\n",
        "\n",
        "# Initialize the routing system\n",
        "route_to_agent = create_routing_system()\n",
        "print(\"   ‚úÖ Intelligent Routing System: Direct image routing + OpenAI decision chain\")\n",
        "logger.info(\"Intelligent routing system successfully initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== LANGGRAPH WORKFLOW CONSTRUCTION ==========\n",
        "# This section constructs the complete multi-agent medical workflow using LangGraph\n",
        "# It integrates all agents, routing logic, validation, and guardrails into a cohesive system\n",
        "\n",
        "def create_medical_agent_workflow(all_agents):\n",
        "    \"\"\"\n",
        "    Creates the complete LangGraph workflow for medical agent orchestration.\n",
        "    This workflow integrates all 6 specialized agents with intelligent routing, \n",
        "    confidence-based decisions, and human validation requirements.\n",
        "    \n",
        "    Returns:\n",
        "        StateGraph: Compiled LangGraph workflow ready for medical consultations\n",
        "    \"\"\"\n",
        "    print(\"üèóÔ∏è Constructing Medical Agent Orchestration Workflow...\")\n",
        "    \n",
        "    # Import LangGraph components\n",
        "    from langgraph.graph import StateGraph, END\n",
        "    from langgraph.checkpoint.memory import MemorySaver\n",
        "    \n",
        "    # Initialize workflow memory for conversation persistence\n",
        "    memory = MemorySaver()\n",
        "    workflow = StateGraph(AgentState)\n",
        "    \n",
        "    # ========== WORKFLOW NODE DEFINITIONS ==========\n",
        "    \n",
        "    def analyze_input_node(state):\n",
        "        \"\"\"Entry point: Analyzes user input and applies initial guardrails\"\"\"\n",
        "        print(\"üîç STEP 1: Analyzing user input and applying guardrails...\")\n",
        "        \n",
        "        # Extract user input\n",
        "        current_input = state.get(\"current_input\", \"\")\n",
        "        input_text = current_input if isinstance(current_input, str) else current_input.get(\"text\", \"\")\n",
        "        \n",
        "        # Apply input guardrails\n",
        "        is_safe, filtered_input, warning = validation_system['apply_input_guardrails'](input_text)\n",
        "        \n",
        "        if not is_safe:\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": warning,\n",
        "                \"agent_name\": \"INPUT_GUARDRAILS\",\n",
        "                \"bypass_routing\": True\n",
        "            }\n",
        "        \n",
        "        # Check for medical images\n",
        "        has_image = isinstance(current_input, dict) and \"image_path\" in current_input\n",
        "        image_type = None\n",
        "        \n",
        "        if has_image:\n",
        "            print(\"üì∏ Medical image detected - Analyzing image type...\")\n",
        "            # Simulate image classification (in real implementation, use GPT-4o Vision)\n",
        "            image_path = current_input.get(\"image_path\", \"\")\n",
        "            if \"brain\" in image_path.lower() or \"mri\" in image_path.lower():\n",
        "                image_type = \"BRAIN MRI\"\n",
        "            elif \"chest\" in image_path.lower() or \"xray\" in image_path.lower():\n",
        "                image_type = \"CHEST X-RAY\"\n",
        "            elif \"skin\" in image_path.lower() or \"lesion\" in image_path.lower():\n",
        "                image_type = \"SKIN LESION\"\n",
        "            else:\n",
        "                image_type = \"OTHER\"\n",
        "        \n",
        "        return {\n",
        "            **state,\n",
        "            \"has_image\": has_image,\n",
        "            \"image_type\": image_type,\n",
        "            \"bypass_routing\": False\n",
        "        }\n",
        "    \n",
        "    def intelligent_routing_node(state):\n",
        "        \"\"\"Smart routing: Direct image routing or OpenAI-powered decision making\"\"\"\n",
        "        print(\"üéØ STEP 2: Intelligent agent routing...\")\n",
        "        \n",
        "        has_image = state.get(\"has_image\", False)\n",
        "        image_type = state.get(\"image_type\", \"\")\n",
        "        \n",
        "        # DIRECT ROUTING FOR MEDICAL IMAGES (Cost optimization + Speed)\n",
        "        if has_image and image_type:\n",
        "            print(f\"üöÄ Direct routing for medical image: {image_type}\")\n",
        "            \n",
        "            if \"BRAIN MRI\" in image_type.upper():\n",
        "                return {**state, \"agent_name\": \"BRAIN_TUMOR_AGENT\", \"next_agent\": \"BRAIN_TUMOR_AGENT\"}\n",
        "            elif \"CHEST X-RAY\" in image_type.upper():\n",
        "                return {**state, \"agent_name\": \"CHEST_XRAY_AGENT\", \"next_agent\": \"CHEST_XRAY_AGENT\"}\n",
        "            elif \"SKIN LESION\" in image_type.upper():\n",
        "                return {**state, \"agent_name\": \"SKIN_LESION_AGENT\", \"next_agent\": \"SKIN_LESION_AGENT\"}\n",
        "            else:\n",
        "                return {**state, \"agent_name\": \"CONVERSATION_AGENT\", \"next_agent\": \"CONVERSATION_AGENT\"}\n",
        "        \n",
        "        # OPENAI DECISION CHAIN FOR TEXT QUERIES\n",
        "        print(\"ü§ñ Using OpenAI decision chain for text query routing...\")\n",
        "        current_input = state.get(\"current_input\", \"\")\n",
        "        input_text = current_input if isinstance(current_input, str) else current_input.get(\"text\", \"\")\n",
        "        \n",
        "        # Simple rule-based routing for demo (in real implementation, use OpenAI)\n",
        "        input_lower = input_text.lower()\n",
        "        \n",
        "        if any(word in input_lower for word in ['recent', 'latest', 'current', 'news', 'outbreak']):\n",
        "            agent_decision = \"WEB_SEARCH_PROCESSOR_AGENT\"\n",
        "        elif any(word in input_lower for word in ['brain tumor', 'covid', 'medical literature', 'research']):\n",
        "            agent_decision = \"RAG_AGENT\"\n",
        "        else:\n",
        "            agent_decision = \"CONVERSATION_AGENT\"\n",
        "        \n",
        "        print(f\"üìã Routing decision: {agent_decision}\")\n",
        "        return {**state, \"agent_name\": agent_decision, \"next_agent\": agent_decision}\n",
        "    \n",
        "    # ========== WORKFLOW EDGE FUNCTIONS ==========\n",
        "    \n",
        "    def check_bypass_routing(state):\n",
        "        \"\"\"Determines if normal routing should be bypassed due to guardrails\"\"\"\n",
        "        return \"apply_final_guardrails\" if state.get(\"bypass_routing\", False) else \"intelligent_routing\"\n",
        "    \n",
        "    def route_to_next_agent(state):\n",
        "        \"\"\"Routes to the selected agent based on routing decision\"\"\"\n",
        "        return state.get(\"next_agent\", \"CONVERSATION_AGENT\")\n",
        "    \n",
        "    def check_rag_confidence(state):\n",
        "        \"\"\"Checks RAG confidence and routes to web search if needed\"\"\"\n",
        "        return validation_system['route_based_on_confidence'](state)\n",
        "    \n",
        "    def check_human_validation_needed(state):\n",
        "        \"\"\"Determines if human validation is required\"\"\"\n",
        "        needs_validation = state.get(\"needs_human_validation\", False)\n",
        "        return \"human_validation\" if needs_validation else \"apply_final_guardrails\"\n",
        "    \n",
        "    # ========== ADD WORKFLOW NODES ==========\n",
        "    \n",
        "    print(\"üìã Adding workflow nodes...\")\n",
        "    \n",
        "    # Core workflow nodes\n",
        "    workflow.add_node(\"analyze_input\", analyze_input_node)\n",
        "    workflow.add_node(\"intelligent_routing\", intelligent_routing_node)\n",
        "    \n",
        "    # Agent execution nodes (using our previously created agents)\n",
        "    workflow.add_node(\"CONVERSATION_AGENT\", all_agents[\"CONVERSATION_AGENT\"])\n",
        "    workflow.add_node(\"RAG_AGENT\", all_agents[\"RAG_AGENT\"])\n",
        "    workflow.add_node(\"WEB_SEARCH_PROCESSOR_AGENT\", all_agents[\"WEB_SEARCH_PROCESSOR_AGENT\"])\n",
        "    workflow.add_node(\"BRAIN_TUMOR_AGENT\", all_agents[\"BRAIN_TUMOR_AGENT\"])\n",
        "    workflow.add_node(\"CHEST_XRAY_AGENT\", all_agents[\"CHEST_XRAY_AGENT\"])\n",
        "    workflow.add_node(\"SKIN_LESION_AGENT\", all_agents[\"SKIN_LESION_AGENT\"])\n",
        "    \n",
        "    # Validation and guardrails nodes\n",
        "    workflow.add_node(\"check_validation\", lambda state: {**state, \"needs_human_validation\": state.get(\"needs_human_validation\", False)})\n",
        "    workflow.add_node(\"human_validation\", lambda state: {\n",
        "        **state, \n",
        "        \"output\": validation_system['create_human_validation_prompt'](\n",
        "            state.get(\"output\", \"\"), \n",
        "            state.get(\"agent_name\", \"\")\n",
        "        )\n",
        "    })\n",
        "    workflow.add_node(\"apply_final_guardrails\", lambda state: {\n",
        "    **state,\n",
        "    \"output\": validation_system['apply_output_guardrails'](\n",
        "        state[\"output\"], state[\"current_input\"]\n",
        "    )\n",
        "})\n",
        "    \n",
        "    # ========== DEFINE WORKFLOW EDGES ==========\n",
        "    \n",
        "    print(\"üîó Defining workflow connections...\")\n",
        "    \n",
        "    # Set entry point\n",
        "    workflow.set_entry_point(\"analyze_input\")\n",
        "    \n",
        "    # Input analysis to routing (with bypass check)\n",
        "    workflow.add_conditional_edges(\n",
        "        \"analyze_input\",\n",
        "        check_bypass_routing,\n",
        "        {\n",
        "            \"intelligent_routing\": \"intelligent_routing\",\n",
        "            \"apply_final_guardrails\": \"apply_final_guardrails\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Routing to specific agents\n",
        "    workflow.add_conditional_edges(\n",
        "        \"intelligent_routing\",\n",
        "        route_to_next_agent,\n",
        "        {\n",
        "            \"CONVERSATION_AGENT\": \"CONVERSATION_AGENT\",\n",
        "            \"RAG_AGENT\": \"RAG_AGENT\", \n",
        "            \"WEB_SEARCH_PROCESSOR_AGENT\": \"WEB_SEARCH_PROCESSOR_AGENT\",\n",
        "            \"BRAIN_TUMOR_AGENT\": \"BRAIN_TUMOR_AGENT\",\n",
        "            \"CHEST_XRAY_AGENT\": \"CHEST_XRAY_AGENT\",\n",
        "            \"SKIN_LESION_AGENT\": \"SKIN_LESION_AGENT\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Agent outputs to validation checks\n",
        "    workflow.add_edge(\"CONVERSATION_AGENT\", \"check_validation\")\n",
        "    workflow.add_edge(\"WEB_SEARCH_PROCESSOR_AGENT\", \"check_validation\")\n",
        "    workflow.add_edge(\"BRAIN_TUMOR_AGENT\", \"check_validation\")\n",
        "    workflow.add_edge(\"CHEST_XRAY_AGENT\", \"check_validation\")\n",
        "    workflow.add_edge(\"SKIN_LESION_AGENT\", \"check_validation\")\n",
        "    \n",
        "    # RAG agent with confidence-based routing\n",
        "    workflow.add_conditional_edges(\n",
        "        \"RAG_AGENT\",\n",
        "        check_rag_confidence,\n",
        "        {\n",
        "            \"WEB_SEARCH_PROCESSOR_AGENT\": \"WEB_SEARCH_PROCESSOR_AGENT\",\n",
        "            \"check_validation\": \"check_validation\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Validation check to human validation or final guardrails\n",
        "    workflow.add_conditional_edges(\n",
        "        \"check_validation\",\n",
        "        check_human_validation_needed,\n",
        "        {\n",
        "            \"human_validation\": \"human_validation\",\n",
        "            \"apply_final_guardrails\": \"apply_final_guardrails\"\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Final steps\n",
        "    workflow.add_edge(\"human_validation\", \"apply_final_guardrails\")\n",
        "    workflow.add_edge(\"apply_final_guardrails\", END)\n",
        "    \n",
        "    # ========== COMPILE WORKFLOW ==========\n",
        "    \n",
        "    print(\"‚öôÔ∏è Compiling LangGraph workflow...\")\n",
        "    compiled_workflow = workflow.compile(checkpointer=memory)\n",
        "    \n",
        "    print(\"‚úÖ Medical Agent Orchestration Workflow successfully created!\")\n",
        "    print(\"üîß Workflow includes:\")\n",
        "    print(\"   - 6 Specialized Medical Agents\")\n",
        "    print(\"   - Intelligent Routing System\") \n",
        "    print(\"   - Confidence-Based Decision Making\")\n",
        "    print(\"   - Human Validation Workflows\")\n",
        "    print(\"   - Input/Output Guardrails\")\n",
        "    print(\"   - Conversation Memory Persistence\")\n",
        "    \n",
        "    return compiled_workflow\n",
        "\n",
        "# Create the complete medical workflow\n",
        "# medical_workflow = create_medical_agent_workflow(all_agents)  # Will execute after agents are combined\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Specialized Agent Implementations\n",
        "\n",
        "This section implements all six specialized medical agents that form the core of our multi-agent system. Each agent is designed with specific medical expertise and integrates seamlessly with the LangGraph orchestration framework.\n",
        "\n",
        "### Agent Architecture Principles:\n",
        "\n",
        "#### **4.1 Clinical Specialization**\n",
        "- **Domain Expertise**: Each agent focuses on specific medical domains\n",
        "- **Evidence-Based Responses**: Grounded in established medical knowledge\n",
        "- **Safety Protocols**: Built-in validation and oversight requirements\n",
        "\n",
        "#### **4.2 Technical Integration**\n",
        "- **Consistent Interfaces**: Standardized input/output formats across agents\n",
        "- **Error Handling**: Graceful failure recovery and user feedback\n",
        "- **Performance Monitoring**: Execution time tracking and optimization\n",
        "\n",
        "#### **4.3 Quality Assurance**\n",
        "- **Confidence Scoring**: Quantitative reliability assessment\n",
        "- **Source Attribution**: Transparent reference to medical literature\n",
        "- **Human Validation**: Medical diagnosis oversight requirements\n",
        "\n",
        "### Medical Agent Catalog:\n",
        "\n",
        "1. **CONVERSATION_AGENT**: General medical consultation and patient interaction\n",
        "2. **RAG_AGENT**: Evidence-based responses from medical literature\n",
        "3. **WEB_SEARCH_PROCESSOR_AGENT**: Current medical information retrieval\n",
        "4. **BRAIN_TUMOR_AGENT**: YOLO-based MRI tumor detection\n",
        "5. **CHEST_XRAY_AGENT**: DenseNet COVID-19 chest X-ray analysis\n",
        "6. **SKIN_LESION_AGENT**: U-Net skin lesion segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,702 - AgentOrchestration - INFO - Text-based medical agents successfully initialized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Conversation Agent: General medical consultation and interaction\n",
            "   ‚úÖ RAG Agent: Medical literature retrieval with confidence scoring\n"
          ]
        }
      ],
      "source": [
        "def create_specialized_agents():\n",
        "    \"\"\"\n",
        "    Create all six specialized medical agents with comprehensive functionality.\n",
        "    \n",
        "    Each agent is designed for specific medical tasks while maintaining\n",
        "    consistent interfaces and quality standards across the system.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing all agent functions\n",
        "    \"\"\"\n",
        "    \n",
        "    def run_conversation_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        General medical conversation agent for patient interaction and consultation.\n",
        "        \n",
        "        This agent handles general medical questions, patient education,\n",
        "        and conversational interactions while maintaining clinical accuracy\n",
        "        and professional standards.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with conversation context\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with conversation response\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing CONVERSATION_AGENT\")\n",
        "        print(f\"üí¨ Selected Agent: CONVERSATION_AGENT\")\n",
        "        \n",
        "        messages = state[\"messages\"]\n",
        "        current_input = state[\"current_input\"]\n",
        "        \n",
        "        # Extract input text for processing\n",
        "        input_text = \"\"\n",
        "        if isinstance(current_input, str):\n",
        "            input_text = current_input\n",
        "        elif isinstance(current_input, dict):\n",
        "            input_text = current_input.get(\"text\", \"\")\n",
        "        \n",
        "        # Build conversation context from message history\n",
        "        recent_context = \"\"\n",
        "        for msg in messages:  # Include complete conversation history\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                recent_context += f\"User: {msg.content}\\n\"\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                recent_context += f\"Assistant: {msg.content}\\n\"\n",
        "        \n",
        "        # Construct comprehensive conversation prompt\n",
        "        conversation_prompt = f\"\"\"User query: {input_text}\n",
        "\n",
        "Recent conversation context: {recent_context}\n",
        "\n",
        "You are an AI-powered Medical Conversation Assistant. Your goal is to facilitate smooth and informative \n",
        "conversations with users, handling both casual and medical-related queries. You must respond naturally \n",
        "while ensuring medical accuracy and clarity.\n",
        "\n",
        "### Role & Capabilities\n",
        "- Engage in **general conversation** while maintaining professionalism.\n",
        "- Answer **medical questions** using verified knowledge.\n",
        "- Route **complex queries** to RAG (retrieval-augmented generation) or web search if needed.\n",
        "- Handle **follow-up questions** while keeping track of conversation context.\n",
        "- Redirect **medical images** to the appropriate AI analysis agent.\n",
        "\n",
        "### Guidelines for Responding:\n",
        "1. **General Conversations:**\n",
        "   - If the user engages in casual talk (e.g., greetings, small talk), respond in a friendly, engaging manner.\n",
        "   - Keep responses **concise and engaging**, unless a detailed answer is needed.\n",
        "\n",
        "2. **Medical Questions:**\n",
        "   - If you have **high confidence** in answering, provide a medically accurate response.\n",
        "   - Ensure responses are **clear, concise, and factual**.\n",
        "\n",
        "3. **Follow-Up & Clarifications:**\n",
        "   - Maintain conversation history for better responses.\n",
        "   - If a query is unclear, ask **follow-up questions** before answering.\n",
        "\n",
        "4. **Handling Medical Image Analysis:**\n",
        "   - Do **not** attempt to analyze images yourself.\n",
        "   - If user speaks about analyzing or processing or detecting or segmenting or classifying any disease \n",
        "     from any image, ask the user to upload the image so that in the next turn it is routed to the \n",
        "     appropriate medical vision agents.\n",
        "   - If an image was uploaded, it would have been routed to the medical computer vision agents. \n",
        "     Read the history to know about the diagnosis results and continue conversation if user asks \n",
        "     anything regarding the diagnosis.\n",
        "   - After processing, **help the user interpret the results**.\n",
        "\n",
        "5. **Uncertainty & Ethical Considerations:**\n",
        "   - If unsure, **never assume** medical facts.\n",
        "   - Recommend consulting a **licensed healthcare professional** for serious medical concerns.\n",
        "   - Avoid providing **medical diagnoses** or **prescriptions**‚Äîstick to general knowledge.\n",
        "\n",
        "### Response Format:\n",
        "- Maintain a **conversational yet professional tone**.\n",
        "- Use **bullet points or numbered lists** for clarity when needed.\n",
        "- If pulling from external sources (RAG/Web Search), mention **where the information is from** \n",
        "  (e.g., \"According to Mayo Clinic...\").\n",
        "- If a user asks for a diagnosis, remind them to **seek medical consultation**.\n",
        "\n",
        "Conversational LLM Response:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            # Generate response using conversation LLM\n",
        "            start_time = time.time()\n",
        "            response = config.conversation.llm.invoke(conversation_prompt)\n",
        "            processing_time = time.time() - start_time\n",
        "            \n",
        "            logger.info(f\"Conversation response generated in {processing_time:.2f}s\")\n",
        "            print(f\"   ‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "            print(f\"   üìù Response Length: {len(response.content)} characters\")\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": response,\n",
        "                \"agent_name\": \"CONVERSATION_AGENT\"\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Conversation agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I apologize, but I'm experiencing technical difficulties. Please try your question again.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"CONVERSATION_AGENT\"\n",
        "            }\n",
        "    \n",
        "    def run_rag_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Medical knowledge retrieval agent using RAG (Retrieval-Augmented Generation).\n",
        "        \n",
        "        This agent searches through pre-ingested medical literature to provide\n",
        "        evidence-based responses with source attribution and confidence scoring.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with query context\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with RAG response and confidence metrics\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing RAG_AGENT\")\n",
        "        print(f\"üìö Selected Agent: RAG_AGENT (Medical Literature Retrieval)\")\n",
        "        \n",
        "        try:\n",
        "            # Initialize RAG system\n",
        "            rag_agent = MedicalRAG(config)\n",
        "            \n",
        "            messages = state[\"messages\"]\n",
        "            query = state[\"current_input\"]\n",
        "            rag_context_limit = config.rag.context_limit\n",
        "            \n",
        "            # Build conversation context for RAG query\n",
        "            recent_context = \"\"\n",
        "            for msg in messages[-rag_context_limit:]:\n",
        "                if isinstance(msg, HumanMessage):\n",
        "                    recent_context += f\"User: {msg.content}\\n\"\n",
        "                elif isinstance(msg, AIMessage):\n",
        "                    recent_context += f\"Assistant: {msg.content}\\n\"\n",
        "            \n",
        "            # Process query through RAG system\n",
        "            start_time = time.time()\n",
        "            response = rag_agent.process_query(query, chat_history=recent_context)\n",
        "            processing_time = time.time() - start_time\n",
        "            \n",
        "            retrieval_confidence = response.get(\"confidence\", 0.0)\n",
        "            sources = response.get(\"sources\", [])\n",
        "            \n",
        "            logger.info(f\"RAG query processed in {processing_time:.2f}s with confidence {retrieval_confidence:.2f}\")\n",
        "            print(f\"   ‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "            print(f\"   üìä Retrieval Confidence: {retrieval_confidence:.2f}\")\n",
        "            print(f\"   üìñ Sources Found: {len(sources)}\")\n",
        "            \n",
        "            # Extract response content\n",
        "            response_content = response[\"response\"]\n",
        "            if hasattr(response_content, 'content'):\n",
        "                response_text = response_content.content\n",
        "            else:\n",
        "                response_text = str(response_content)\n",
        "            \n",
        "            # Check for insufficient information indicators\n",
        "            insufficient_info = False\n",
        "            insufficient_indicators = [\n",
        "                \"don't have enough information\",\n",
        "                \"not enough information\",\n",
        "                \"insufficient information\",\n",
        "                \"cannot answer\",\n",
        "                \"unable to answer\"\n",
        "            ]\n",
        "            \n",
        "            if isinstance(response_text, str):\n",
        "                for indicator in insufficient_indicators:\n",
        "                    if indicator in response_text.lower():\n",
        "                        insufficient_info = True\n",
        "                        logger.info(\"RAG response indicates insufficient information\")\n",
        "                        print(f\"   ‚ö†Ô∏è Information Gap: Insufficient local knowledge detected\")\n",
        "                        break\n",
        "            \n",
        "            # Create response message\n",
        "            if retrieval_confidence >= config.rag.min_retrieval_confidence:\n",
        "                response_output = AIMessage(content=response_text)\n",
        "            else:\n",
        "                response_output = AIMessage(content=\"\")\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": response_output,\n",
        "                \"needs_human_validation\": False,\n",
        "                \"retrieval_confidence\": retrieval_confidence,\n",
        "                \"agent_name\": \"RAG_AGENT\",\n",
        "                \"insufficient_info\": insufficient_info\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"RAG agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I'm having trouble accessing medical literature right now. Please try again or ask about current medical information.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"RAG_AGENT\",\n",
        "                \"retrieval_confidence\": 0.0,\n",
        "                \"insufficient_info\": True\n",
        "            }\n",
        "    \n",
        "    print(\"   ‚úÖ Conversation Agent: General medical consultation and interaction\")\n",
        "    print(\"   ‚úÖ RAG Agent: Medical literature retrieval with confidence scoring\")\n",
        "    \n",
        "    return {\n",
        "        \"CONVERSATION_AGENT\": run_conversation_agent,\n",
        "        \"RAG_AGENT\": run_rag_agent\n",
        "    }\n",
        "\n",
        "# Initialize conversation and knowledge agents\n",
        "text_agents = create_specialized_agents()\n",
        "logger.info(\"Text-based medical agents successfully initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,712 - AgentOrchestration - INFO - Web search and brain tumor agents successfully initialized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Web Search Agent: Current medical information retrieval\n",
            "   ‚úÖ Brain Tumor Agent: YOLO-based MRI tumor detection\n"
          ]
        }
      ],
      "source": [
        "def create_web_search_and_vision_agents():\n",
        "    \"\"\"\n",
        "    Create web search and medical vision agents for comprehensive medical analysis.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing web search and vision agent functions\n",
        "    \"\"\"\n",
        "    \n",
        "    def run_web_search_processor_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Web search processor agent for current medical information retrieval.\n",
        "        \n",
        "        This agent handles queries about recent medical developments, current\n",
        "        health situations, and time-sensitive medical information using web search.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with query context\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with web search results\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing WEB_SEARCH_PROCESSOR_AGENT\")\n",
        "        print(f\"üåê Selected Agent: WEB_SEARCH_PROCESSOR_AGENT (Current Medical Info)\")\n",
        "        \n",
        "        try:\n",
        "            messages = state[\"messages\"]\n",
        "            web_search_context_limit = config.web_search.context_limit\n",
        "            \n",
        "            # Build conversation context for web search\n",
        "            recent_context = \"\"\n",
        "            for msg in messages[-web_search_context_limit:]:\n",
        "                if isinstance(msg, HumanMessage):\n",
        "                    recent_context += f\"User: {msg.content}\\n\"\n",
        "                elif isinstance(msg, AIMessage):\n",
        "                    recent_context += f\"Assistant: {msg.content}\\n\"\n",
        "            \n",
        "            # Initialize web search processor\n",
        "            web_search_processor = WebSearchProcessorAgent(config)\n",
        "            \n",
        "            # Process query with web search\n",
        "            start_time = time.time()\n",
        "            processed_response = web_search_processor.process_web_search_results(\n",
        "                query=state[\"current_input\"], \n",
        "                chat_history=recent_context\n",
        "            )\n",
        "            processing_time = time.time() - start_time\n",
        "            \n",
        "            logger.info(f\"Web search completed in {processing_time:.2f}s\")\n",
        "            print(f\"   ‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "            \n",
        "            # FIX: Extract content from AIMessage object before using len()\n",
        "            if hasattr(processed_response, 'content'):\n",
        "                response_content = processed_response.content\n",
        "                response_length = len(response_content)\n",
        "            else:\n",
        "                response_content = str(processed_response)\n",
        "                response_length = len(response_content)\n",
        "                \n",
        "            print(f\"   üìù Response Length: {response_length} characters\")\n",
        "            \n",
        "            # Determine involved agents for attribution\n",
        "            if state['agent_name']:\n",
        "                involved_agents = f\"{state['agent_name']}, WEB_SEARCH_PROCESSOR_AGENT\"\n",
        "            else:\n",
        "                involved_agents = \"WEB_SEARCH_PROCESSOR_AGENT\"\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": AIMessage(content=response_content),  # ‚úÖ FIXED\n",
        "                \"agent_name\": involved_agents\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Web search agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I'm having trouble accessing current medical information. Please try again or ask about established medical knowledge.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"WEB_SEARCH_PROCESSOR_AGENT\"\n",
        "            }\n",
        "    \n",
        "    def run_brain_tumor_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Brain tumor detection agent using YOLO model for MRI analysis.\n",
        "        \n",
        "        This agent processes brain MRI images to detect and localize tumors\n",
        "        using state-of-the-art YOLO object detection with medical visualization.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with image input\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with tumor detection results\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing BRAIN_TUMOR_AGENT\")\n",
        "        print(f\"üß† Selected Agent: BRAIN_TUMOR_AGENT (YOLO Tumor Detection)\")\n",
        "        \n",
        "        try:\n",
        "            current_input = state[\"current_input\"]\n",
        "            image_path = None\n",
        "\n",
        "            if isinstance(current_input, dict):\n",
        "                # Try both possible key names\n",
        "                image_path = current_input.get(\"image\") or current_input.get(\"image_path\")\n",
        "            elif isinstance(current_input, str):\n",
        "                # If current_input is a string, it might be the image path directly\n",
        "                image_path = current_input\n",
        "            \n",
        "            if not image_path:\n",
        "                raise ValueError(\"No image path provided for brain tumor analysis\")\n",
        "            \n",
        "            print(f\"   üì∏ Analyzing MRI image: {os.path.basename(image_path)}\")\n",
        "            \n",
        "            # Initialize image analyzer\n",
        "            image_analyzer = AgentConfig.get_image_analyzer()\n",
        "            \n",
        "            # Perform tumor detection\n",
        "            start_time = time.time()\n",
        "            predicted_class = image_analyzer.classify_brain_tumor(image_path)\n",
        "            detection_time = time.time() - start_time\n",
        "            \n",
        "            # Generate visualization\n",
        "            visualization_success = image_analyzer.visualize_brain_tumor_detection(image_path)\n",
        "            \n",
        "            logger.info(f\"Brain tumor analysis completed in {detection_time:.2f}s: {predicted_class}\")\n",
        "            print(f\"   ‚è±Ô∏è Analysis Time: {detection_time:.2f} seconds\")\n",
        "            print(f\"   üéØ Detection Result: {predicted_class}\")\n",
        "            print(f\"   üìä Visualization: {'‚úÖ Generated' if visualization_success else '‚ùå Failed'}\")\n",
        "            \n",
        "            # Generate clinical response\n",
        "            if predicted_class == \"tumor_detected\":\n",
        "                response = AIMessage(content=\"**BRAIN TUMOR ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"üö® **TUMOR DETECTED**: The YOLO analysis of the uploaded brain MRI image indicates the presence of a potential brain tumor. \" +\n",
        "                                   \"The AI system has identified suspicious regions that require immediate medical attention.\\n\\n\" +\n",
        "                                   \"**‚ö†Ô∏è IMPORTANT**: This AI analysis is for preliminary screening only and must be validated by a qualified radiologist or neurologist. \" +\n",
        "                                   \"Please consult with a healthcare professional immediately for proper diagnosis and treatment planning.\")\n",
        "            elif predicted_class == \"no_tumor\":\n",
        "                response = AIMessage(content=\"**BRAIN TUMOR ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚úÖ **NO TUMOR DETECTED**: The YOLO analysis of the uploaded brain MRI image indicates no obvious tumor structures. \" +\n",
        "                                   \"The scan appears to show normal brain tissue patterns.\\n\\n\" +\n",
        "                                   \"**üìã NOTE**: While no tumor was detected by the AI system, this does not replace professional medical evaluation. \" +\n",
        "                                   \"Clinical correlation with symptoms and professional radiological interpretation is recommended.\")\n",
        "            else:\n",
        "                response = AIMessage(content=\"**BRAIN TUMOR ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚ö†Ô∏è **ANALYSIS INCONCLUSIVE**: The uploaded image could not be properly analyzed by the AI system. \" +\n",
        "                                   \"This may be due to image quality, format issues, or the image not being a brain MRI scan.\\n\\n\" +\n",
        "                                   \"**üìã RECOMMENDATION**: Please ensure the image is a clear brain MRI scan and try again, or consult with a medical professional.\")\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": response,\n",
        "                \"needs_human_validation\": True,  # All medical diagnoses require validation\n",
        "                \"agent_name\": \"BRAIN_TUMOR_AGENT\"\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Brain tumor agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I encountered an error while analyzing the brain MRI image. Please ensure the image is a clear MRI scan and try again.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"BRAIN_TUMOR_AGENT\"\n",
        "            }\n",
        "    \n",
        "    print(\"   ‚úÖ Web Search Agent: Current medical information retrieval\")\n",
        "    print(\"   ‚úÖ Brain Tumor Agent: YOLO-based MRI tumor detection\")\n",
        "    \n",
        "    return {\n",
        "        \"WEB_SEARCH_PROCESSOR_AGENT\": run_web_search_processor_agent,\n",
        "        \"BRAIN_TUMOR_AGENT\": run_brain_tumor_agent\n",
        "    }\n",
        "\n",
        "# Initialize web search and brain tumor agents\n",
        "web_and_brain_agents = create_web_search_and_vision_agents()\n",
        "logger.info(\"Web search and brain tumor agents successfully initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Completing Medical Vision Agents\n",
        "\n",
        "This section completes our medical vision agent implementations with the chest X-ray and skin lesion analysis capabilities. These agents represent the pinnacle of AI-assisted medical imaging analysis.\n",
        "\n",
        "### Advanced Medical Vision Capabilities:\n",
        "\n",
        "#### **5.1 Chest X-ray COVID-19 Detection**\n",
        "- **DenseNet-121 Architecture**: State-of-the-art convolutional neural network\n",
        "- **COVID-19 Pattern Recognition**: Trained on validated radiological datasets\n",
        "- **Clinical Correlation**: Results aligned with radiological findings\n",
        "- **Quality Assurance**: Confidence scoring and validation requirements\n",
        "\n",
        "#### **5.2 Skin Lesion Segmentation**\n",
        "- **U-Net Architecture**: Gold standard for medical image segmentation\n",
        "- **Pixel-Level Precision**: Exact lesion boundary delineation\n",
        "- **Dermatological Applications**: ABCDE rule assessment support\n",
        "- **Visualization**: Advanced overlay plotting for clinical interpretation\n",
        "\n",
        "### Clinical Safety Framework:\n",
        "All medical vision agents implement comprehensive safety protocols:\n",
        "- **Human Validation Requirements**: Medical diagnoses flagged for oversight\n",
        "- **Confidence Thresholds**: Statistical reliability assessment\n",
        "- **Error Handling**: Graceful failure recovery and user guidance\n",
        "- **Ethical Guidelines**: Appropriate disclaimers and professional referrals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,725 - AgentOrchestration - INFO - All medical vision agents successfully initialized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Chest X-ray Agent: DenseNet COVID-19 detection with clinical recommendations\n",
            "   ‚úÖ Skin Lesion Agent: U-Net segmentation with dermatological analysis\n",
            "\n",
            "üéØ All Six Specialized Medical Agents Successfully Created:\n",
            "   üí¨ CONVERSATION_AGENT - General medical consultation\n",
            "   üìö RAG_AGENT - Medical literature retrieval\n",
            "   üåê WEB_SEARCH_PROCESSOR_AGENT - Current medical information\n",
            "   üß† BRAIN_TUMOR_AGENT - YOLO MRI tumor detection\n",
            "   ü´Å CHEST_XRAY_AGENT - DenseNet COVID-19 detection\n",
            "   üë§ SKIN_LESION_AGENT - U-Net lesion segmentation\n"
          ]
        }
      ],
      "source": [
        "def create_remaining_vision_agents():\n",
        "    \"\"\"\n",
        "    Create the remaining medical vision agents for comprehensive image analysis.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing chest X-ray and skin lesion agent functions\n",
        "    \"\"\"\n",
        "    \n",
        "    def run_chest_xray_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Chest X-ray analysis agent using DenseNet for COVID-19 detection.\n",
        "        \n",
        "        This agent processes chest X-ray images to detect COVID-19 patterns\n",
        "        using a DenseNet-121 convolutional neural network trained on validated\n",
        "        radiological datasets with confirmed diagnoses.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with image input\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with COVID-19 detection results\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing CHEST_XRAY_AGENT\")\n",
        "        print(f\"ü´Å Selected Agent: CHEST_XRAY_AGENT (DenseNet COVID-19 Detection)\")\n",
        "        \n",
        "        try:\n",
        "            current_input = state[\"current_input\"]\n",
        "            image_path = None\n",
        "\n",
        "            if isinstance(current_input, dict):\n",
        "                image_path = current_input.get(\"image\") or current_input.get(\"image_path\")\n",
        "            elif isinstance(current_input, str):\n",
        "                image_path = current_input\n",
        "            \n",
        "            if not image_path:\n",
        "                raise ValueError(\"No image path provided for chest X-ray analysis\")\n",
        "            \n",
        "            print(f\"   üì∏ Analyzing chest X-ray: {os.path.basename(image_path)}\")\n",
        "            \n",
        "            # Initialize image analyzer\n",
        "            image_analyzer = AgentConfig.get_image_analyzer()\n",
        "            \n",
        "            # Perform COVID-19 classification\n",
        "            start_time = time.time()\n",
        "            predicted_class = image_analyzer.classify_chest_xray(image_path)\n",
        "            classification_time = time.time() - start_time\n",
        "            \n",
        "            logger.info(f\"Chest X-ray analysis completed in {classification_time:.2f}s: {predicted_class}\")\n",
        "            print(f\"   ‚è±Ô∏è Analysis Time: {classification_time:.2f} seconds\")\n",
        "            print(f\"   üéØ Classification Result: {predicted_class}\")\n",
        "            \n",
        "            # Generate clinical response based on classification\n",
        "            if \"covid19\" in str(predicted_class).lower():\n",
        "                response = AIMessage(content=\"**CHEST X-RAY ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"üö® **COVID-19 POSITIVE**: The DenseNet analysis of the uploaded chest X-ray image indicates patterns consistent with COVID-19 pneumonia. \" +\n",
        "                                   \"The AI system has detected radiological features commonly associated with COVID-19 infection.\\n\\n\" +\n",
        "                                   \"**üìã CLINICAL RECOMMENDATIONS:**\\n\" +\n",
        "                                   \"‚Ä¢ Immediate isolation protocols should be initiated\\n\" +\n",
        "                                   \"‚Ä¢ RT-PCR confirmation testing is strongly recommended\\n\" +\n",
        "                                   \"‚Ä¢ Clinical correlation with symptoms and exposure history\\n\" +\n",
        "                                   \"‚Ä¢ Contact tracing procedures as per local guidelines\\n\\n\" +\n",
        "                                   \"**‚ö†Ô∏è IMPORTANT**: This AI analysis is for preliminary screening only and must be validated by a qualified radiologist. \" +\n",
        "                                   \"Please consult with a healthcare professional immediately for proper diagnosis and treatment planning.\")\n",
        "            elif \"normal\" in str(predicted_class).lower():\n",
        "                response = AIMessage(content=\"**CHEST X-RAY ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚úÖ **COVID-19 NEGATIVE**: The DenseNet analysis of the uploaded chest X-ray image indicates normal lung patterns with no obvious signs of COVID-19 pneumonia. \" +\n",
        "                                   \"The radiological features appear consistent with healthy lung tissue.\\n\\n\" +\n",
        "                                   \"**üìã CLINICAL NOTES:**\\n\" +\n",
        "                                   \"‚Ä¢ No obvious COVID-19 radiological signs detected\\n\" +\n",
        "                                   \"‚Ä¢ Continue standard clinical assessment if symptomatic\\n\" +\n",
        "                                   \"‚Ä¢ Consider alternative diagnoses if respiratory symptoms persist\\n\" +\n",
        "                                   \"‚Ä¢ Follow local health guidelines for testing if indicated\\n\\n\" +\n",
        "                                   \"**üìã NOTE**: While no COVID-19 patterns were detected, this does not rule out infection entirely. \" +\n",
        "                                   \"Clinical correlation and professional medical evaluation are recommended, especially if symptoms are present.\")\n",
        "            else:\n",
        "                response = AIMessage(content=\"**CHEST X-RAY ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚ö†Ô∏è **ANALYSIS INCONCLUSIVE**: The uploaded image could not be properly classified by the AI system. \" +\n",
        "                                   \"This may be due to image quality issues, incorrect format, or the image not being a chest X-ray.\\n\\n\" +\n",
        "                                   \"**üìã RECOMMENDATIONS:**\\n\" +\n",
        "                                   \"‚Ä¢ Ensure the image is a clear chest X-ray (PA or AP view)\\n\" +\n",
        "                                   \"‚Ä¢ Check image quality and format\\n\" +\n",
        "                                   \"‚Ä¢ Consider re-uploading with better image quality\\n\" +\n",
        "                                   \"‚Ä¢ Consult with a medical professional for proper evaluation\")\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": response,\n",
        "                \"needs_human_validation\": True,  # All medical diagnoses require validation\n",
        "                \"agent_name\": \"CHEST_XRAY_AGENT\"\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Chest X-ray agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I encountered an error while analyzing the chest X-ray image. Please ensure the image is a clear chest X-ray and try again.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"CHEST_XRAY_AGENT\"\n",
        "            }\n",
        "    \n",
        "    def run_skin_lesion_agent(state: AgentState) -> AgentState:\n",
        "        \"\"\"\n",
        "        Skin lesion segmentation agent using U-Net for dermatological analysis.\n",
        "        \n",
        "        This agent processes dermoscopic images to perform precise pixel-level\n",
        "        segmentation of skin lesions using U-Net architecture optimized for\n",
        "        medical image analysis.\n",
        "        \n",
        "        Args:\n",
        "            state: Current agent state with image input\n",
        "            \n",
        "        Returns:\n",
        "            Updated state with skin lesion segmentation results\n",
        "        \"\"\"\n",
        "        logger.info(\"Executing SKIN_LESION_AGENT\")\n",
        "        print(f\"üë§ Selected Agent: SKIN_LESION_AGENT (U-Net Lesion Segmentation)\")\n",
        "        \n",
        "        try:\n",
        "            current_input = state[\"current_input\"]\n",
        "            image_path = None\n",
        "\n",
        "            if isinstance(current_input, dict):\n",
        "                image_path = current_input.get(\"image\") or current_input.get(\"image_path\")\n",
        "            elif isinstance(current_input, str):\n",
        "                image_path = current_input\n",
        "            \n",
        "            if not image_path:\n",
        "                raise ValueError(\"No image path provided for skin lesion analysis\")\n",
        "            \n",
        "            print(f\"   üì∏ Analyzing skin lesion: {os.path.basename(image_path)}\")\n",
        "            \n",
        "            # Initialize image analyzer\n",
        "            image_analyzer = AgentConfig.get_image_analyzer()\n",
        "            \n",
        "            # Perform lesion segmentation\n",
        "            start_time = time.time()\n",
        "            segmentation_result = image_analyzer.segment_skin_lesion(image_path)\n",
        "            segmentation_time = time.time() - start_time\n",
        "            \n",
        "            logger.info(f\"Skin lesion segmentation completed in {segmentation_time:.2f}s: {segmentation_result}\")\n",
        "            print(f\"   ‚è±Ô∏è Analysis Time: {segmentation_time:.2f} seconds\")\n",
        "            print(f\"   üéØ Segmentation Result: {segmentation_result}\")\n",
        "            \n",
        "            # Check segmentation success\n",
        "            segmentation_success = False\n",
        "            if isinstance(segmentation_result, bool):\n",
        "                segmentation_success = segmentation_result\n",
        "            elif isinstance(segmentation_result, str):\n",
        "                segmentation_success = any(indicator in segmentation_result.lower() \n",
        "                                         for indicator in ['success', 'completed', 'true'])\n",
        "            \n",
        "            # Generate clinical response based on segmentation results\n",
        "            if segmentation_success:\n",
        "                response = AIMessage(content=\"**SKIN LESION ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚úÖ **SEGMENTATION COMPLETED**: The U-Net analysis has successfully segmented the uploaded skin lesion image with pixel-level precision. \" +\n",
        "                                   \"The AI system has identified and delineated the lesion boundaries for detailed morphological analysis.\\n\\n\" +\n",
        "                                   \"**üìä DERMATOLOGICAL APPLICATIONS:**\\n\" +\n",
        "                                   \"‚Ä¢ **Lesion Area Measurement**: Precise quantification for tracking changes over time\\n\" +\n",
        "                                   \"‚Ä¢ **Border Irregularity Assessment**: Analysis of lesion edge characteristics\\n\" +\n",
        "                                   \"‚Ä¢ **Asymmetry Analysis**: Evaluation of lesion symmetry patterns\\n\" +\n",
        "                                   \"‚Ä¢ **Color Distribution Mapping**: Assessment of pigmentation variations\\n\" +\n",
        "                                   \"‚Ä¢ **Follow-up Comparison**: Baseline for future monitoring studies\\n\\n\" +\n",
        "                                   \"**üîç ABCDE RULE SUPPORT:**\\n\" +\n",
        "                                   \"The segmentation provides quantitative data to support evaluation of:\\n\" +\n",
        "                                   \"‚Ä¢ **A**symmetry: Morphological analysis of lesion shape\\n\" +\n",
        "                                   \"‚Ä¢ **B**order: Edge irregularity and definition assessment\\n\" +\n",
        "                                   \"‚Ä¢ **C**olor: Pigmentation distribution mapping\\n\" +\n",
        "                                   \"‚Ä¢ **D**iameter: Precise area and perimeter measurements\\n\" +\n",
        "                                   \"‚Ä¢ **E**volution: Baseline for monitoring changes\\n\\n\" +\n",
        "                                   \"**‚ö†Ô∏è CLINICAL NOTE**: This AI segmentation is a diagnostic aid and requires validation by a qualified dermatologist. \" +\n",
        "                                   \"Please consult with a healthcare professional for proper diagnosis and treatment recommendations.\")\n",
        "            else:\n",
        "                response = AIMessage(content=\"**SKIN LESION ANALYSIS RESULTS**\\n\\n\" +\n",
        "                                   \"‚ö†Ô∏è **SEGMENTATION INCOMPLETE**: The uploaded image could not be properly segmented by the AI system. \" +\n",
        "                                   \"This may be due to image quality issues, incorrect format, or the image not being a clear skin lesion.\\n\\n\" +\n",
        "                                   \"**üìã RECOMMENDATIONS:**\\n\" +\n",
        "                                   \"‚Ä¢ Ensure the image is a clear dermoscopic or clinical photo of a skin lesion\\n\" +\n",
        "                                   \"‚Ä¢ Check image quality, lighting, and focus\\n\" +\n",
        "                                   \"‚Ä¢ Avoid images with excessive hair, artifacts, or poor contrast\\n\" +\n",
        "                                   \"‚Ä¢ Consider re-uploading with better image quality\\n\" +\n",
        "                                   \"‚Ä¢ Consult with a dermatologist for professional evaluation\\n\\n\" +\n",
        "                                   \"**üè• ALTERNATIVE**: If you have concerns about a skin lesion, please schedule an appointment with a dermatologist for proper clinical examination.\")\n",
        "            \n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": response,\n",
        "                \"needs_human_validation\": True,  # All medical diagnoses require validation\n",
        "                \"agent_name\": \"SKIN_LESION_AGENT\"\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Skin lesion agent error: {str(e)}\")\n",
        "            error_response = AIMessage(content=\"I encountered an error while analyzing the skin lesion image. Please ensure the image is a clear dermatological photo and try again.\")\n",
        "            return {\n",
        "                **state,\n",
        "                \"output\": error_response,\n",
        "                \"agent_name\": \"SKIN_LESION_AGENT\"\n",
        "            }\n",
        "    \n",
        "    print(\"   ‚úÖ Chest X-ray Agent: DenseNet COVID-19 detection with clinical recommendations\")\n",
        "    print(\"   ‚úÖ Skin Lesion Agent: U-Net segmentation with dermatological analysis\")\n",
        "    \n",
        "    return {\n",
        "        \"CHEST_XRAY_AGENT\": run_chest_xray_agent,\n",
        "        \"SKIN_LESION_AGENT\": run_skin_lesion_agent\n",
        "    }\n",
        "\n",
        "# Initialize remaining vision agents\n",
        "vision_agents = create_remaining_vision_agents()\n",
        "logger.info(\"All medical vision agents successfully initialized\")\n",
        "print(\"\\nüéØ All Six Specialized Medical Agents Successfully Created:\")\n",
        "print(\"   üí¨ CONVERSATION_AGENT - General medical consultation\")\n",
        "print(\"   üìö RAG_AGENT - Medical literature retrieval\")\n",
        "print(\"   üåê WEB_SEARCH_PROCESSOR_AGENT - Current medical information\")\n",
        "print(\"   üß† BRAIN_TUMOR_AGENT - YOLO MRI tumor detection\")\n",
        "print(\"   ü´Å CHEST_XRAY_AGENT - DenseNet COVID-19 detection\")\n",
        "print(\"   üë§ SKIN_LESION_AGENT - U-Net lesion segmentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,729 - AgentOrchestration - INFO - All 6 medical agents successfully integrated\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Agent Integration Status:\n",
            "   ‚úÖ Conversation Agent\n",
            "   ‚úÖ Rag Agent\n",
            "   ‚úÖ Web Search Processor Agent\n",
            "   ‚úÖ Brain Tumor Agent\n",
            "   ‚úÖ Chest Xray Agent\n",
            "   ‚úÖ Skin Lesion Agent\n",
            "üèóÔ∏è Creating Medical Agent Workflow...\n",
            "üèóÔ∏è Constructing Medical Agent Orchestration Workflow...\n",
            "üìã Adding workflow nodes...\n",
            "üîó Defining workflow connections...\n",
            "‚öôÔ∏è Compiling LangGraph workflow...\n",
            "‚úÖ Medical Agent Orchestration Workflow successfully created!\n",
            "üîß Workflow includes:\n",
            "   - 6 Specialized Medical Agents\n",
            "   - Intelligent Routing System\n",
            "   - Confidence-Based Decision Making\n",
            "   - Human Validation Workflows\n",
            "   - Input/Output Guardrails\n",
            "   - Conversation Memory Persistence\n",
            "‚úÖ Medical workflow ready for consultations\\!\n"
          ]
        }
      ],
      "source": [
        "# Combine all agent functions into a single dictionary for workflow integration\n",
        "all_agents = {\n",
        "    **text_agents,\n",
        "    **web_and_brain_agents, \n",
        "    **vision_agents\n",
        "}\n",
        "\n",
        "print(\"üîÑ Agent Integration Status:\")\n",
        "for agent_name in all_agents.keys():\n",
        "    print(f\"   ‚úÖ {agent_name.replace('run_', '').replace('_', ' ').title()}\")\n",
        "\n",
        "logger.info(f\"All {len(all_agents)} medical agents successfully integrated\")\n",
        "\n",
        "# Create the complete medical workflow now that all agents are defined\n",
        "print(\"üèóÔ∏è Creating Medical Agent Workflow...\")\n",
        "medical_workflow = create_medical_agent_workflow(all_agents)\n",
        "print(\"‚úÖ Medical workflow ready for consultations\\!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ°Ô∏è Initializing Medical AI Safety and Validation System...\n",
            "‚úÖ Validation and Guardrails System successfully initialized!\n",
            "üìã Available functions:\n",
            "   - check_medical_confidence\n",
            "   - apply_input_guardrails\n",
            "   - apply_output_guardrails\n",
            "   - create_human_validation_prompt\n",
            "   - route_based_on_confidence\n"
          ]
        }
      ],
      "source": [
        "# ========== GUARDRAILS AND VALIDATION SYSTEM ==========\n",
        "# This section implements the safety and validation components that ensure medical outputs are safe and validated by humans when necessary\n",
        "\n",
        "def create_validation_and_guardrails_system():\n",
        "    \"\"\"\n",
        "    Creates the validation and guardrails system for medical AI responses.\n",
        "    This system ensures safety, ethical compliance, and human oversight for medical diagnoses.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing validation and guardrails functions\n",
        "    \"\"\"\n",
        "    print(\"üõ°Ô∏è Initializing Medical AI Safety and Validation System...\")\n",
        "    \n",
        "    def check_medical_confidence(state):\n",
        "        \"\"\"\n",
        "        Evaluates the confidence of medical predictions and determines if human validation is needed.\n",
        "        \n",
        "        Args:\n",
        "            state (dict): Current agent state with prediction confidence scores\n",
        "            \n",
        "        Returns:\n",
        "            dict: Updated state with human validation requirements\n",
        "        \"\"\"\n",
        "        confidence_threshold = 0.85  # High confidence threshold for medical decisions\n",
        "        \n",
        "        # Extract confidence from different agent outputs\n",
        "        confidence_score = state.get('retrieval_confidence', 0.0)\n",
        "        agent_name = state.get('agent_name', '')\n",
        "        \n",
        "        # Medical image analysis agents always require human validation\n",
        "        medical_agents = ['BRAIN_TUMOR_AGENT', 'CHEST_XRAY_AGENT', 'SKIN_LESION_AGENT']\n",
        "        \n",
        "        if any(agent in agent_name for agent in medical_agents):\n",
        "            print(f\"üè• Medical diagnosis detected from {agent_name} - Human validation REQUIRED\")\n",
        "            state['needs_human_validation'] = True\n",
        "        elif confidence_score < confidence_threshold:\n",
        "            print(f\"‚ö†Ô∏è Low confidence score ({confidence_score:.2f}) - Human validation recommended\")\n",
        "            state['needs_human_validation'] = True\n",
        "        else:\n",
        "            print(f\"‚úÖ High confidence score ({confidence_score:.2f}) - No additional validation needed\")\n",
        "            state['needs_human_validation'] = False\n",
        "            \n",
        "        return state\n",
        "    \n",
        "    def apply_input_guardrails(user_input):\n",
        "        \"\"\"\n",
        "        Applies safety filters to user input to prevent harmful or inappropriate queries.\n",
        "        \n",
        "        Args:\n",
        "            user_input (str): Raw user input text\n",
        "            \n",
        "        Returns:\n",
        "            tuple: (is_safe, filtered_input, warning_message)\n",
        "        \"\"\"\n",
        "        print(\"üîç Applying input safety guardrails...\")\n",
        "        \n",
        "        # Define prohibited content patterns\n",
        "        prohibited_patterns = [\n",
        "            'suicide', 'self-harm', 'kill myself', 'end my life',\n",
        "            'illegal drugs', 'prescription fraud', 'fake prescription',\n",
        "            'medical advice for others', 'diagnose someone else'\n",
        "        ]\n",
        "        \n",
        "        # Check for prohibited content\n",
        "        input_lower = user_input.lower()\n",
        "        for pattern in prohibited_patterns:\n",
        "            if pattern in input_lower:\n",
        "                warning_msg = f\"‚ö†Ô∏è Input contains potentially harmful content: '{pattern}'. Please rephrase your query.\"\n",
        "                print(f\"üö´ Blocked input: Contains '{pattern}'\")\n",
        "                return False, \"\", warning_msg\n",
        "        \n",
        "        # Input passes safety checks\n",
        "        print(\"‚úÖ Input passes safety checks\")\n",
        "        return True, user_input, \"\"\n",
        "    \n",
        "    def apply_output_guardrails(generated_response, original_query):\n",
        "        \"\"\"\n",
        "        Applies safety filters and ethical guidelines to AI-generated medical responses.\n",
        "        \n",
        "        Args:\n",
        "            generated_response (str): AI-generated medical response\n",
        "            original_query (str): Original user query for context\n",
        "            \n",
        "        Returns:\n",
        "            str: Sanitized and ethically compliant response\n",
        "        \"\"\"\n",
        "        print(\"üõ°Ô∏è Applying output safety and ethical guardrails...\")\n",
        "\n",
        "        # FIX: Extract content from AIMessage object if needed\n",
        "        if hasattr(generated_response, 'content'):\n",
        "            response_content = generated_response.content\n",
        "        else:\n",
        "            response_content = str(generated_response)\n",
        "        \n",
        "        # Ensure medical disclaimers are present\n",
        "        medical_keywords = ['diagnosis', 'treatment', 'medication', 'symptoms', 'disease', 'condition']\n",
        "        \n",
        "        if any(keyword in response_content.lower() for keyword in medical_keywords): \n",
        "            disclaimer = \"\\n\\n‚öïÔ∏è **Medical Disclaimer**: This AI analysis is for informational purposes only and should not replace professional medical advice. Always consult with a qualified healthcare provider for medical decisions.\"\n",
        "            \n",
        "            if disclaimer not in response_content:\n",
        "                response_content += disclaimer\n",
        "                print(\"üìã Added medical disclaimer to response\")\n",
        "        \n",
        "        # Remove overly confident diagnostic language\n",
        "        confidence_phrases = [\n",
        "            (\"you definitely have\", \"you may have\"),\n",
        "            (\"this is certainly\", \"this could be\"),\n",
        "            (\"you should immediately\", \"consider consulting a healthcare provider about\"),\n",
        "            (\"this proves\", \"this suggests\")\n",
        "        ]\n",
        "        \n",
        "        for problematic, replacement in confidence_phrases:\n",
        "            if problematic in response_content.lower():\n",
        "                response_content = response_content.replace(problematic, replacement)\n",
        "                print(f\"üîÑ Replaced overly confident language: '{problematic}' ‚Üí '{replacement}'\")\n",
        "\n",
        "        from langchain_core.messages import AIMessage\n",
        "        return AIMessage(content=response_content)\n",
        "        \n",
        "        #print(\"‚úÖ Output passes ethical and safety guidelines\")\n",
        "        #return response_content\n",
        "    \n",
        "    def create_human_validation_prompt(medical_output, agent_name):\n",
        "        \"\"\"\n",
        "        Creates a human validation prompt for medical professionals.\n",
        "        \n",
        "        Args:\n",
        "            medical_output (str): AI-generated medical analysis\n",
        "            agent_name (str): Name of the agent that generated the output\n",
        "            \n",
        "        Returns:\n",
        "            str: Formatted validation prompt for human review\n",
        "        \"\"\"\n",
        "        print(f\"üë®‚Äç‚öïÔ∏è Creating human validation prompt for {agent_name}\")\n",
        "        \n",
        "        validation_prompt = f\"\"\"\n",
        "üè• **MEDICAL AI VALIDATION REQUIRED**\n",
        "\n",
        "**AI Agent**: {agent_name}\n",
        "**Analysis Output**: {medical_output}\n",
        "\n",
        "**For Healthcare Professionals**:\n",
        "Please review this AI-generated medical analysis and validate its accuracy:\n",
        "- ‚úÖ **APPROVE**: If the analysis appears medically sound and appropriate\n",
        "- ‚ùå **REJECT**: If you identify any medical inaccuracies or concerns\n",
        "- üí¨ **COMMENTS**: Provide any additional clinical insights or corrections\n",
        "\n",
        "**For Patients**:\n",
        "This analysis has been generated by AI and requires validation by a healthcare professional.\n",
        "Please seek consultation with a qualified medical provider for proper diagnosis and treatment.\n",
        "\n",
        "**Validation Status**: ‚è≥ PENDING HUMAN REVIEW\n",
        "        \"\"\"\n",
        "        \n",
        "        return validation_prompt\n",
        "    \n",
        "    def route_based_on_confidence(state):\n",
        "        \"\"\"\n",
        "        Implements intelligent routing based on confidence scores and information sufficiency.\n",
        "        \n",
        "        Args:\n",
        "            state (dict): Current workflow state\n",
        "            \n",
        "        Returns:\n",
        "            str: Next agent to route to ('WEB_SEARCH_PROCESSOR_AGENT' or 'END')\n",
        "        \"\"\"\n",
        "        confidence = state.get('retrieval_confidence', 0.0)\n",
        "        insufficient_info = state.get('insufficient_info', False)\n",
        "        min_confidence = 0.75  # Minimum confidence threshold\n",
        "        \n",
        "        print(f\"üéØ Routing decision - Confidence: {confidence:.2f}, Insufficient info: {insufficient_info}\")\n",
        "        \n",
        "        if confidence < min_confidence or insufficient_info:\n",
        "            print(\"üîÑ Low confidence/insufficient info ‚Üí Routing to Web Search Agent\")\n",
        "            return \"WEB_SEARCH_PROCESSOR_AGENT\"\n",
        "        else:\n",
        "            print(\"‚úÖ High confidence ‚Üí Proceeding to validation check\")\n",
        "            return \"check_validation\"\n",
        "    \n",
        "    # Return all validation and guardrails functions\n",
        "    return {\n",
        "        'check_medical_confidence': check_medical_confidence,\n",
        "        'apply_input_guardrails': apply_input_guardrails,\n",
        "        'apply_output_guardrails': apply_output_guardrails,\n",
        "        'create_human_validation_prompt': create_human_validation_prompt,\n",
        "        'route_based_on_confidence': route_based_on_confidence\n",
        "    }\n",
        "\n",
        "# Initialize the validation and guardrails system\n",
        "validation_system = create_validation_and_guardrails_system()\n",
        "\n",
        "print(\"‚úÖ Validation and Guardrails System successfully initialized!\")\n",
        "print(\"üìã Available functions:\")\n",
        "for func_name in validation_system.keys():\n",
        "    print(f\"   - {func_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========== INDIVIDUAL AGENT TESTING ==========\n",
        "\n",
        "## Individual Agent Testing Suite\n",
        "\n",
        "This section provides separate test cells for each medical agent, allowing you to:\n",
        "- **Test agents individually** without running the full 19-test suite\n",
        "- **Debug specific agent functionality** in isolation\n",
        "- **Quickly verify agent responses** during development\n",
        "- **Understand each agent's capabilities** and output format\n",
        "\n",
        "### Available Individual Tests:\n",
        "\n",
        "1. **ü§ñ Conversation Agent** - General medical consultation and patient interaction\n",
        "2. **üìö RAG Agent** - Medical literature retrieval from knowledge base\n",
        "3. **üåê Web Search Agent** - Current medical information and recent developments\n",
        "4. **üß† Brain Tumor Agent** - MRI brain tumor detection using YOLO\n",
        "5. **ü´Å Chest X-ray Agent** - COVID-19 detection using DenseNet\n",
        "6. **ü©∫ Skin Lesion Agent** - Skin lesion segmentation using U-Net\n",
        "\n",
        "### Usage Instructions:\n",
        "\n",
        "- **Run cells individually** by clicking the run button on each test cell\n",
        "- **Modify test queries** in each cell to test different scenarios\n",
        "- **Check agent outputs** and response times for each component\n",
        "- **Use for debugging** when specific agents aren't working as expected\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:25,766 - AgentOrchestration - INFO - Executing WEB_SEARCH_PROCESSOR_AGENT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ TESTING CONVERSATION AGENT\n",
            "==================================================\n",
            "üìù Test Query: Hello\\! I'm experiencing mild headaches recently. What could be causing them?\n",
            "üéØ Expected Agent: CONVERSATION_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "ü§ñ Using OpenAI decision chain for text query routing...\n",
            "üìã Routing decision: WEB_SEARCH_PROCESSOR_AGENT\n",
            "üåê Selected Agent: WEB_SEARCH_PROCESSOR_AGENT (Current Medical Info)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:26,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:57:38,745 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:57:38,748 - AgentOrchestration - INFO - Web search completed in 12.98s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚è±Ô∏è Processing Time: 12.98 seconds\n",
            "   üìù Response Length: 994 characters\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "üìã Added medical disclaimer to response\n",
            "‚úÖ CONVERSATION AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: WEB_SEARCH_PROCESSOR_AGENT, WEB_SEARCH_PROCESSOR_AGENT\n",
            "‚è±Ô∏è Processing Time: 12.99 seconds\n",
            "üìè Response Length: 1201 characters\n",
            "üí¨ Agent Response:\n",
            "--------------------------------------------------\n",
            "Mild headaches can be caused by a variety of factors. Hormonal changes, such as those experienced during pregnancy, can lead to headaches. Lifestyle factors like stress, depression, alcohol use, skipping meals, changes in sleep patterns, and taking too much medication can also trigger headaches. Environmental factors, including exposure to secondhand smoke, strong smells, allergens, certain foods, pollution, noise, lighting, and weather changes, can be potential triggers. Illnesses, including infections, colds, fevers, sinusitis, throat or ear infections, can cause headaches as well. In some cases, headaches can result from physical strain or a blow to the head. Rarely, they can indicate a more serious medical problem. Headaches can also be a symptom of dehydration, alcohol consumption, or sinus problems. Genetics may also play a role in the development of headaches. If your headaches persist, it's recommended to consult a healthcare provider for a proper diagnosis and treatment.\n",
            "\n",
            "‚öïÔ∏è **Medical Disclaimer**: This AI analysis is for informational purposes only and should not replace professional medical advice. Always consult with a qualified healthcare provider for medical decisions.\n",
            "--------------------------------------------------\n",
            "üéØ ‚ùå INCORRECT AGENT: Expected CONVERSATION_AGENT, got WEB_SEARCH_PROCESSOR_AGENT, WEB_SEARCH_PROCESSOR_AGENT\n"
          ]
        }
      ],
      "source": [
        "# ========== CONVERSATION AGENT INDIVIDUAL TEST ==========\n",
        "print(\"ü§ñ TESTING CONVERSATION AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query - modify this to test different conversation scenarios\n",
        "test_query = \"Hello\\! I'm experiencing mild headaches recently. What could be causing them?\"\n",
        "\n",
        "print(f\"üìù Test Query: {test_query}\")\n",
        "print(f\"üéØ Expected Agent: CONVERSATION_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query,\n",
        "    \"has_image\": False,\n",
        "    \"image_type\": None,\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"conversation_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ CONVERSATION AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üí¨ Agent Response:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"CONVERSATION_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ùå INCORRECT AGENT: Expected CONVERSATION_AGENT, got {selected_agent}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during conversation agent test:\")\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(f\"Type: {type(e).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:38,770 - AgentOrchestration - INFO - Executing RAG_AGENT\n",
            "2025-08-11 14:57:38,771 - agents.rag_agent - INFO - Initializing Medical RAG system\n",
            "2025-08-11 14:57:38,771 - agents.rag_agent.doc_parser - INFO - Medical Document Parser initialized!\n",
            "2025-08-11 14:57:38,788 - AgentOrchestration - ERROR - RAG agent error: Storage folder ./data/qdrant_db is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.\n",
            "2025-08-11 14:57:38,789 - AgentOrchestration - INFO - Executing WEB_SEARCH_PROCESSOR_AGENT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìö TESTING RAG AGENT\n",
            "==================================================\n",
            "üìù Test Query: What are the different types of brain tumors and their characteristics?\n",
            "üéØ Expected Agent: RAG_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "ü§ñ Using OpenAI decision chain for text query routing...\n",
            "üìã Routing decision: RAG_AGENT\n",
            "üìö Selected Agent: RAG_AGENT (Medical Literature Retrieval)\n",
            "üéØ Routing decision - Confidence: 0.00, Insufficient info: True\n",
            "üîÑ Low confidence/insufficient info ‚Üí Routing to Web Search Agent\n",
            "üåê Selected Agent: WEB_SEARCH_PROCESSOR_AGENT (Current Medical Info)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:57:39,561 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 503 Service Unavailable\"\n",
            "2025-08-11 14:57:39,563 - openai._base_client - INFO - Retrying request to /chat/completions in 0.468685 seconds\n",
            "2025-08-11 14:57:41,229 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:57:53,281 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:57:53,285 - AgentOrchestration - INFO - Web search completed in 14.49s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚è±Ô∏è Processing Time: 14.49 seconds\n",
            "   üìù Response Length: 1117 characters\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "‚úÖ RAG AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: RAG_AGENT, WEB_SEARCH_PROCESSOR_AGENT\n",
            "‚è±Ô∏è Processing Time: 14.52 seconds\n",
            "üìè Response Length: 1117 characters\n",
            "üîç Retrieval Confidence: 0.000\n",
            "üìö RAG Agent Response:\n",
            "--------------------------------------------------\n",
            "Brain tumors are categorized into more than 150 types, based on their location, the type of cells they are made of, and whether they are cancerous (malignant) or noncancerous (benign). They can be primary, originating in the brain, or metastatic, spreading from cancer elsewhere in the body. \n",
            "\n",
            "Benign brain tumors are typically slow-growing, while malignant ones tend to grow fast. Some common benign brain tumors include meningioma and dysembryoplastic neuroepithelial tumor, which is usually found in children and teens and can cause seizures. \n",
            "\n",
            "Approximately 78% of malignant brain tumors are gliomas, which develop in glial cells that assist nerve cells. Medulloblastomas, another type of malignant tumor, are fast-growing, form at the base of the skull, and are the most common cancerous brain tumor in children. Glioblastoma is the most serious type of brain tumor and is becoming more common as the population ages. \n",
            "\n",
            "Brain tumors can also form in the spinal cord or column and can affect anyone, though they are slightly more common in males. The only exception is meningioma, which is more common in females.\n",
            "--------------------------------------------------\n",
            "üéØ ‚úÖ CORRECT AGENT SELECTED\n"
          ]
        }
      ],
      "source": [
        "# ========== RAG AGENT INDIVIDUAL TEST ==========\n",
        "print(\"üìö TESTING RAG AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query - modify this to test different knowledge retrieval scenarios\n",
        "test_query = \"What are the different types of brain tumors and their characteristics?\"\n",
        "\n",
        "print(f\"üìù Test Query: {test_query}\")\n",
        "print(f\"üéØ Expected Agent: RAG_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query,\n",
        "    \"has_image\": False,\n",
        "    \"image_type\": None,\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"rag_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    retrieval_confidence = result.get(\"retrieval_confidence\", 0.0)\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ RAG AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üîç Retrieval Confidence: {retrieval_confidence:.3f}\")\n",
        "    print(f\"üìö RAG Agent Response:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"RAG_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ö†Ô∏è AGENT: Expected RAG_AGENT, got {selected_agent}\")\n",
        "        print(\"   (May have been routed to Web Search due to low confidence)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during RAG agent test:\")\n",
        "    print(f\"   Error: {str(e)}\")\n",
        "    print(f\"   Type: {type(e).__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:58:38,928 - AgentOrchestration - INFO - Executing WEB_SEARCH_PROCESSOR_AGENT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê TESTING WEB SEARCH PROCESSOR AGENT\n",
            "==================================================\n",
            "üìù Test Query: What are the latest developments in COVID-19 treatment in 2025?\n",
            "üéØ Expected Agent: WEB_SEARCH_PROCESSOR_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "ü§ñ Using OpenAI decision chain for text query routing...\n",
            "üìã Routing decision: WEB_SEARCH_PROCESSOR_AGENT\n",
            "üåê Selected Agent: WEB_SEARCH_PROCESSOR_AGENT (Current Medical Info)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:58:40,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:58:56,821 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-08-11 14:58:56,825 - AgentOrchestration - INFO - Web search completed in 17.90s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚è±Ô∏è Processing Time: 17.90 seconds\n",
            "   üìù Response Length: 1531 characters\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "üìã Added medical disclaimer to response\n",
            "‚úÖ WEB SEARCH PROCESSOR AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: WEB_SEARCH_PROCESSOR_AGENT, WEB_SEARCH_PROCESSOR_AGENT\n",
            "‚è±Ô∏è Processing Time: 17.91 seconds\n",
            "üìè Response Length: 1738 characters\n",
            "üåê Web Search Response:\n",
            "--------------------------------------------------\n",
            "The latest developments in COVID-19 treatment in 2025 focus on understanding and treating the long-term effects of the virus, often referred to as Long COVID. The RECOVER Initiative is using data-driven research to study these effects, with recent studies focusing on cardiovascular and kidney function, as well as gastrointestinal outcomes in children and adolescents post-COVID.\n",
            "\n",
            "In terms of treatment, ongoing trials are looking at the use of antivirals like valacyclovir, Truvada, and Maraviroc for Long COVID. Other potential treatments being tested include immunomodulators, JAK inhibitors, checkpoint inhibitors, FCRN inhibitors, and B cell depletion therapies. Antivirals like Paxlovid might help reduce the replication of the virus and potentially mitigate Long COVID. For immunocompromised individuals, pre-exposure prophylaxis with Pemgarda may help reduce acute, severe symptomatic infections and possibly prevent Long COVID, although this has not been tested yet.\n",
            "\n",
            "The World Health Organization (WHO) continues to monitor the effectiveness of approved COVID-19 vaccines against evolving strains of the virus. As of May 2025, vaccines targeting the JN.1 or KP.2 lineages were deemed appropriate. The WHO also provides updated recommendations for the clinical management of people with COVID-19. \n",
            "\n",
            "It's important to note that the situation remains dynamic, with COVID-19 infections growing in many areas as of August 2025. Ongoing surveillance and research are crucial to managing the pandemic and its long-term effects.\n",
            "\n",
            "‚öïÔ∏è **Medical Disclaimer**: This AI analysis is for informational purposes only and should not replace professional medical advice. Always consult with a qualified healthcare provider for medical decisions.\n",
            "--------------------------------------------------\n",
            "üéØ ‚úÖ CORRECT AGENT SELECTED\n"
          ]
        }
      ],
      "source": [
        "# ========== WEB SEARCH PROCESSOR AGENT INDIVIDUAL TEST ==========\n",
        "print(\"üåê TESTING WEB SEARCH PROCESSOR AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query - modify this to test different web search scenarios\n",
        "test_query = \"What are the latest developments in COVID-19 treatment in 2025?\"\n",
        "\n",
        "print(f\"üìù Test Query: {test_query}\")\n",
        "print(f\"üéØ Expected Agent: WEB_SEARCH_PROCESSOR_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query,\n",
        "    \"has_image\": False,\n",
        "    \"image_type\": None,\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"websearch_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ WEB SEARCH PROCESSOR AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üåê Web Search Response:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"WEB_SEARCH_PROCESSOR_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ùå INCORRECT AGENT: Expected WEB_SEARCH_PROCESSOR_AGENT, got {selected_agent}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during web search agent test:\")\n",
        "    print(f\"   Error: {str(e)}\")\n",
        "    print(f\"   Type: {type(e).__name__}\")\n",
        "    print(f\"   Note: Web search requires valid Tavily API key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:59:10,514 - AgentOrchestration - INFO - Executing BRAIN_TUMOR_AGENT\n",
            "2025-08-11 14:59:10,514 - agents.image_analysis_agent.chest_xray_agent.covid_chest_xray_inference - INFO - Using device: cpu\n",
            "2025-08-11 14:59:10,701 - agents.image_analysis_agent.chest_xray_agent.covid_chest_xray_inference - INFO - Model loaded successfully from ./agents/image_analysis_agent/chest_xray_agent/models/covid_chest_xray_model.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† TESTING BRAIN TUMOR AGENT\n",
            "==================================================\n",
            "üìù Test Query: Please analyze this brain MRI scan for tumor detection\n",
            "üì∏ Image Path: sample_images/brain_tumour/00059_141.jpg\n",
            "üéØ Expected Agent: BRAIN_TUMOR_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üì∏ Medical image detected - Analyzing image type...\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "üöÄ Direct routing for medical image: BRAIN MRI\n",
            "üß† Selected Agent: BRAIN_TUMOR_AGENT (YOLO Tumor Detection)\n",
            "   üì∏ Analyzing MRI image: 00059_141.jpg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:59:10,762 - agents.image_analysis_agent.brain_tumor_agent.brain_tumor_inference - INFO - Brain tumor detection model loaded successfully from ./agents/image_analysis_agent/brain_tumor_agent/models/brain_tumour_od.pt\n",
            "2025-08-11 14:59:10,873 - agents.image_analysis_agent.skin_lesion_agent.skin_lesion_inference - INFO - Model loaded successfully from ./agents/image_analysis_agent/skin_lesion_agent/models/checkpointN25_.pth.tar\n",
            "2025-08-11 14:59:10,874 - agents.image_analysis_agent.brain_tumor_agent.brain_tumor_inference - ERROR - Error during brain tumor detection: Image file not found: sample_images/brain_tumour/00059_141.jpg\n",
            "2025-08-11 14:59:10,874 - agents.image_analysis_agent.brain_tumor_agent.brain_tumor_inference - ERROR - Error generating brain tumor visualization: Image file not found: sample_images/brain_tumour/00059_141.jpg\n",
            "2025-08-11 14:59:10,875 - AgentOrchestration - INFO - Brain tumor analysis completed in 0.00s: error\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚è±Ô∏è Analysis Time: 0.00 seconds\n",
            "   üéØ Detection Result: error\n",
            "   üìä Visualization: ‚ùå Failed\n",
            "üë®‚Äç‚öïÔ∏è Creating human validation prompt for BRAIN_TUMOR_AGENT\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "üìã Added medical disclaimer to response\n",
            "‚úÖ BRAIN TUMOR AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: BRAIN_TUMOR_AGENT\n",
            "‚è±Ô∏è Processing Time: 0.37 seconds\n",
            "üìè Response Length: 1294 characters\n",
            "üè• Needs Human Validation: True\n",
            "üß† Brain Tumor Analysis Result:\n",
            "--------------------------------------------------\n",
            "\n",
            "üè• **MEDICAL AI VALIDATION REQUIRED**\n",
            "\n",
            "**AI Agent**: BRAIN_TUMOR_AGENT\n",
            "**Analysis Output**: content='**BRAIN TUMOR ANALYSIS RESULTS**\\n\\n‚ö†Ô∏è **ANALYSIS INCONCLUSIVE**: The uploaded image could not be properly analyzed by the AI system. This may be due to image quality, format issues, or the image not being a brain MRI scan.\\n\\n**üìã RECOMMENDATION**: Please ensure the image is a clear brain MRI scan and try again, or consult with a medical professional.' additional_kwargs={} response_metadata={}\n",
            "\n",
            "**For Healthcare Professionals**:\n",
            "Please review this AI-generated medical analysis and validate its accuracy:\n",
            "- ‚úÖ **APPROVE**: If the analysis appears medically sound and appropriate\n",
            "- ‚ùå **REJECT**: If you identify any medical inaccuracies or concerns\n",
            "- üí¨ **COMMENTS**: Provide any additional clinical insights or corrections\n",
            "\n",
            "**For Patients**:\n",
            "This analysis has been generated by AI and requires validation by a healthcare professional.\n",
            "Please seek consultation with a qualified medical provider for proper diagnosis and treatment.\n",
            "\n",
            "**Validation Status**: ‚è≥ PENDING HUMAN REVIEW\n",
            "        \n",
            "\n",
            "‚öïÔ∏è **Medical Disclaimer**: This AI analysis is for informational purposes only and should not replace professional medical advice. Always consult with a qualified healthcare provider for medical decisions.\n",
            "--------------------------------------------------\n",
            "üéØ ‚úÖ CORRECT AGENT SELECTED\n",
            "üìä Check uploads/brain_tumor_output/ for visualization images\n"
          ]
        }
      ],
      "source": [
        "# ========== BRAIN TUMOR AGENT INDIVIDUAL TEST ==========\n",
        "print(\"üß† TESTING BRAIN TUMOR AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query with image - modify the image_path to test different brain MRI scans\n",
        "test_query = {\n",
        "    \"text\": \"Please analyze this brain MRI scan for tumor detection\",\n",
        "    \"image_path\": \"sample_images/brain_tumour/00059_141.jpg\"\n",
        "}\n",
        "\n",
        "print(f\"üìù Test Query: {test_query['text']}\")\n",
        "print(f\"üì∏ Image Path: {test_query['image_path']}\")\n",
        "print(f\"üéØ Expected Agent: BRAIN_TUMOR_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query['text'],\n",
        "    \"has_image\": True,\n",
        "    \"image_type\": \"BRAIN MRI\",\n",
        "    \"uploaded_image_path\": test_query['image_path'],\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"brain_tumor_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    needs_validation = result.get(\"needs_human_validation\", False)\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ BRAIN TUMOR AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üè• Needs Human Validation: {needs_validation}\")\n",
        "    print(f\"üß† Brain Tumor Analysis Result:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"BRAIN_TUMOR_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ùå INCORRECT AGENT: Expected BRAIN_TUMOR_AGENT, got {selected_agent}\")\n",
        "    \n",
        "    # Check for visualization output\n",
        "    print(f\"üìä Check uploads/brain_tumor_output/ for visualization images\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during brain tumor agent test:\")\n",
        "    print(f\"   Error: {str(e)}\")\n",
        "    print(f\"   Type: {type(e).__name__}\")\n",
        "    print(f\"   Note: Requires brain MRI image file and YOLO model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:59:20,952 - AgentOrchestration - INFO - Executing CHEST_XRAY_AGENT\n",
            "2025-08-11 14:59:20,953 - agents.image_analysis_agent.chest_xray_agent.covid_chest_xray_inference - ERROR - Error during prediction Covid Chest X-ray: [Errno 2] No such file or directory: 'sample_images/chest_x-ray_covid_and_normal/NORMAL2-IM-0364-0001.jpeg'\n",
            "2025-08-11 14:59:20,953 - AgentOrchestration - INFO - Chest X-ray analysis completed in 0.00s: None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü´Å TESTING CHEST X-RAY AGENT\n",
            "==================================================\n",
            "üìù Test Query: Analyze this chest X-ray for COVID-19 signs\n",
            "üì∏ Image Path: sample_images/chest_x-ray_covid_and_normal/NORMAL2-IM-0364-0001.jpeg\n",
            "üéØ Expected Agent: CHEST_XRAY_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üì∏ Medical image detected - Analyzing image type...\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "üöÄ Direct routing for medical image: CHEST X-RAY\n",
            "ü´Å Selected Agent: CHEST_XRAY_AGENT (DenseNet COVID-19 Detection)\n",
            "   üì∏ Analyzing chest X-ray: NORMAL2-IM-0364-0001.jpeg\n",
            "   ‚è±Ô∏è Analysis Time: 0.00 seconds\n",
            "   üéØ Classification Result: None\n",
            "üë®‚Äç‚öïÔ∏è Creating human validation prompt for CHEST_XRAY_AGENT\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "üìã Added medical disclaimer to response\n",
            "‚úÖ CHEST X-RAY AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: CHEST_XRAY_AGENT\n",
            "‚è±Ô∏è Processing Time: 0.01 seconds\n",
            "üìè Response Length: 1403 characters\n",
            "üè• Needs Human Validation: True\n",
            "ü´Å Chest X-ray Analysis Result:\n",
            "--------------------------------------------------\n",
            "\n",
            "üè• **MEDICAL AI VALIDATION REQUIRED**\n",
            "\n",
            "**AI Agent**: CHEST_XRAY_AGENT\n",
            "**Analysis Output**: content='**CHEST X-RAY ANALYSIS RESULTS**\\n\\n‚ö†Ô∏è **ANALYSIS INCONCLUSIVE**: The uploaded image could not be properly classified by the AI system. This may be due to image quality issues, incorrect format, or the image not being a chest X-ray.\\n\\n**üìã RECOMMENDATIONS:**\\n‚Ä¢ Ensure the image is a clear chest X-ray (PA or AP view)\\n‚Ä¢ Check image quality and format\\n‚Ä¢ Consider re-uploading with better image quality\\n‚Ä¢ Consult with a medical professional for proper evaluation' additional_kwargs={} response_metadata={}\n",
            "\n",
            "**For Healthcare Professionals**:\n",
            "Please review this AI-generated medical analysis and validate its accuracy:\n",
            "- ‚úÖ **APPROVE**: If the analysis appears medically sound and appropriate\n",
            "- ‚ùå **REJECT**: If you identify any medical inaccuracies or concerns\n",
            "- üí¨ **COMMENTS**: Provide any additional clinical insights or corrections\n",
            "\n",
            "**For Patients**:\n",
            "This analysis has been generated by AI and requires validation by a healthcare professional.\n",
            "Please seek consultation with a qualified medical provider for proper diagnosis and treatment.\n",
            "\n",
            "**Validation Status**: ‚è≥ PENDING HUMAN REVIEW\n",
            "        \n",
            "\n",
            "‚öïÔ∏è **Medical Disclaimer**: This AI analysis is for informational purposes only and should not replace professional medical advice. Always consult with a qualified healthcare provider for medical decisions.\n",
            "--------------------------------------------------\n",
            "üéØ ‚úÖ CORRECT AGENT SELECTED\n"
          ]
        }
      ],
      "source": [
        "# ========== CHEST X-RAY AGENT INDIVIDUAL TEST ==========\n",
        "print(\"ü´Å TESTING CHEST X-RAY AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query with image - modify the image_path to test different chest X-ray images\n",
        "test_query = {\n",
        "    \"text\": \"Analyze this chest X-ray for COVID-19 signs\",\n",
        "    \"image_path\": \"sample_images/chest_x-ray_covid_and_normal/NORMAL2-IM-0364-0001.jpeg\"\n",
        "}\n",
        "\n",
        "print(f\"üìù Test Query: {test_query['text']}\")\n",
        "print(f\"üì∏ Image Path: {test_query['image_path']}\")\n",
        "print(f\"üéØ Expected Agent: CHEST_XRAY_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query['text'],\n",
        "    \"has_image\": True,\n",
        "    \"image_type\": \"CHEST X-RAY\",\n",
        "    \"uploaded_image_path\": test_query['image_path'],\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"chest_xray_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    needs_validation = result.get(\"needs_human_validation\", False)\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ CHEST X-RAY AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üè• Needs Human Validation: {needs_validation}\")\n",
        "    print(f\"ü´Å Chest X-ray Analysis Result:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"CHEST_XRAY_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ùå INCORRECT AGENT: Expected CHEST_XRAY_AGENT, got {selected_agent}\")\n",
        "    \n",
        "    # Show classification result\n",
        "    if \"COVID-19 POSITIVE\" in final_output_text:\n",
        "        print(\"ü¶† COVID-19 POSITIVE detected\")\n",
        "    elif \"COVID-19 NEGATIVE\" in final_output_text:\n",
        "        print(\"‚úÖ COVID-19 NEGATIVE (Normal)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during chest X-ray agent test:\")\n",
        "    print(f\"   Error: {str(e)}\")\n",
        "    print(f\"   Type: {type(e).__name__}\")\n",
        "    print(f\"   Note: Requires chest X-ray image file and DenseNet model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-11 14:59:30,178 - AgentOrchestration - INFO - Executing SKIN_LESION_AGENT\n",
            "[ WARN:0@126.575] global loadsave.cpp:268 findDecoder imread_('sample_images/skin_lesion_images/ISIC_0020849.jpg'): can't open/read file: check file path/integrity\n",
            "2025-08-11 14:59:30,181 - agents.image_analysis_agent.skin_lesion_agent.skin_lesion_inference - ERROR - Error during segmentation: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
            "\n",
            "2025-08-11 14:59:30,182 - AgentOrchestration - ERROR - Skin lesion agent error: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü©∫ TESTING SKIN LESION AGENT\n",
            "==================================================\n",
            "üìù Test Query: Segment and analyze this skin lesion image\n",
            "üì∏ Image Path: sample_images/skin_lesion_images/ISIC_0020849.jpg\n",
            "üéØ Expected Agent: SKIN_LESION_AGENT\n",
            "üöÄ Executing workflow...\n",
            "üîç STEP 1: Analyzing user input and applying guardrails...\n",
            "üîç Applying input safety guardrails...\n",
            "‚úÖ Input passes safety checks\n",
            "üì∏ Medical image detected - Analyzing image type...\n",
            "üéØ STEP 2: Intelligent agent routing...\n",
            "üöÄ Direct routing for medical image: SKIN LESION\n",
            "üë§ Selected Agent: SKIN_LESION_AGENT (U-Net Lesion Segmentation)\n",
            "   üì∏ Analyzing skin lesion: ISIC_0020849.jpg\n",
            "üõ°Ô∏è Applying output safety and ethical guardrails...\n",
            "‚úÖ SKIN LESION AGENT TEST COMPLETED\n",
            "üë§ Selected Agent: SKIN_LESION_AGENT\n",
            "‚è±Ô∏è Processing Time: 0.01 seconds\n",
            "üìè Response Length: 132 characters\n",
            "üè• Needs Human Validation: False\n",
            "ü©∫ Skin Lesion Segmentation Result:\n",
            "--------------------------------------------------\n",
            "I encountered an error while analyzing the skin lesion image. Please ensure the image is a clear dermatological photo and try again.\n",
            "--------------------------------------------------\n",
            "üéØ ‚úÖ CORRECT AGENT SELECTED\n",
            "üìä Check uploads/skin_lesion_output/ for segmentation visualization\n"
          ]
        }
      ],
      "source": [
        "# ========== SKIN LESION AGENT INDIVIDUAL TEST ==========\n",
        "print(\"ü©∫ TESTING SKIN LESION AGENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test query with image - modify the image_path to test different skin lesion images\n",
        "test_query = {\n",
        "    \"text\": \"Segment and analyze this skin lesion image\",\n",
        "    \"image_path\": \"sample_images/skin_lesion_images/ISIC_0020849.jpg\"\n",
        "}\n",
        "\n",
        "print(f\"üìù Test Query: {test_query['text']}\")\n",
        "print(f\"üì∏ Image Path: {test_query['image_path']}\")\n",
        "print(f\"üéØ Expected Agent: SKIN_LESION_AGENT\")\n",
        "print(\"üöÄ Executing workflow...\")\n",
        "\n",
        "# Create initial state\n",
        "initial_state = {\n",
        "    \"current_input\": test_query,\n",
        "    \"current_query\": test_query['text'],\n",
        "    \"has_image\": True,\n",
        "    \"image_type\": \"SKIN LESION\",\n",
        "    \"uploaded_image_path\": test_query['image_path'],\n",
        "    \"messages\": [],\n",
        "    \"agent_name\": None,\n",
        "    \"output\": None,\n",
        "    \"needs_human_validation\": False,\n",
        "    \"retrieval_confidence\": 0.0,\n",
        "    \"bypass_routing\": False,\n",
        "    \"insufficient_info\": False\n",
        "}\n",
        "\n",
        "# Execute workflow\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    thread_config = {\"configurable\": {\"thread_id\": \"skin_lesion_test\"}}\n",
        "    result = medical_workflow.invoke(initial_state, thread_config)\n",
        "    \n",
        "    # Process results\n",
        "    selected_agent = result.get(\"agent_name\", \"UNKNOWN\")\n",
        "    final_output = result.get(\"output\", \"No output generated\")\n",
        "    needs_validation = result.get(\"needs_human_validation\", False)\n",
        "    \n",
        "    # Handle AIMessage objects\n",
        "    if hasattr(final_output, 'content'):\n",
        "        final_output_text = final_output.content\n",
        "    else:\n",
        "        final_output_text = str(final_output)\n",
        "    \n",
        "    processing_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ SKIN LESION AGENT TEST COMPLETED\")\n",
        "    print(f\"üë§ Selected Agent: {selected_agent}\")\n",
        "    print(f\"‚è±Ô∏è Processing Time: {processing_time:.2f} seconds\")\n",
        "    print(f\"üìè Response Length: {len(final_output_text)} characters\")\n",
        "    print(f\"üè• Needs Human Validation: {needs_validation}\")\n",
        "    print(f\"ü©∫ Skin Lesion Segmentation Result:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(final_output_text)\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Verify agent selection\n",
        "    if \"SKIN_LESION_AGENT\" in selected_agent:\n",
        "        print(\"üéØ ‚úÖ CORRECT AGENT SELECTED\")\n",
        "    else:\n",
        "        print(f\"üéØ ‚ùå INCORRECT AGENT: Expected SKIN_LESION_AGENT, got {selected_agent}\")\n",
        "    \n",
        "    # Check for segmentation output\n",
        "    print(f\"üìä Check uploads/skin_lesion_output/ for segmentation visualization\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR during skin lesion agent test:\")\n",
        "    print(f\"   Error: {str(e)}\")\n",
        "    print(f\"   Type: {type(e).__name__}\")\n",
        "    print(f\"   Note: Requires skin lesion image file and U-Net model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Individual Agent Testing Complete\\!\n",
        "\n",
        "**You now have 6 separate test cells for individual agent testing:**\n",
        "\n",
        "‚úÖ **Conversation Agent Test** - General medical consultation\n",
        "‚úÖ **RAG Agent Test** - Medical knowledge retrieval\n",
        "‚úÖ **Web Search Agent Test** - Current medical information\n",
        "‚úÖ **Brain Tumor Agent Test** - MRI tumor detection\n",
        "‚úÖ **Chest X-ray Agent Test** - COVID-19 detection\n",
        "‚úÖ **Skin Lesion Agent Test** - Lesion segmentation\n",
        "\n",
        "### Usage Tips:\n",
        "\n",
        "- **Run individually**: Execute each test cell separately to test specific agents\n",
        "- **Modify queries**: Change the test_query in each cell to test different scenarios\n",
        "- **Check outputs**: Look for correct agent selection and response quality\n",
        "- **Debug issues**: Use individual tests to isolate problems with specific agents\n",
        "- **Performance monitoring**: Each test shows processing time and response metrics\n",
        "\n",
        "### Expected Results:\n",
        "\n",
        "- Each test should show \"‚úÖ CORRECT AGENT SELECTED\"\n",
        "- Response times should be reasonable (typically 2-15 seconds)\n",
        "- Medical image agents should indicate \"Needs Human Validation: True\"\n",
        "- Responses should be medically appropriate and well-formatted\n",
        "\n",
        "**Happy Testing\\! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
